{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How TorchCox works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7feb2c669940>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to get our survival data in the right format, which is the staircase encoding described in `notebooks/Staircase_encoding.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>time</th>\n",
       "      <th>status</th>\n",
       "      <th>smoke</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bob</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sally</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>James</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ann</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  time  status  smoke\n",
       "0    Bob     1       1      1\n",
       "1  Sally     3       1      0\n",
       "2  James     6       0      0\n",
       "3    Ann    10       1      1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valdf = pd.DataFrame({'id':['Bob','Sally','James','Ann'], 'time':[1,3,6,10], 'status':[1,1,0,1], 'smoke':[1,0,0,1]})\n",
    "valdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tname = 'time'\n",
    "Xnames = ['smoke']\n",
    "dname = 'status'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There is one difference between the snippet below and what is described in `notebooks/Staircase_encoding.ipynb`: the padding value here is a large negative number instead of zero.**  \n",
    "\n",
    "The reason for this will become clear shortly, but in a nutshell is because we will use tensor-wide operations to compute the likelihood and we do not want these padding values to affect the calculation.  \n",
    "\n",
    "The top row of the tensor of data which contributes to the numerator in the Cox likelihood will never be padding so is unaffected, but the denominator is computed from the full risk set (all rows in a front slice of the tensor) so the padding could affect the result, which would be a serious problem.  \n",
    "\n",
    "As we will see the denominator of the Cox likelihood involves a `logsumexp()` function, so a large negative padding value results in `exp()` underflowing to zero, then being fed into a `sum()` where these zeros do not affect the result, and voilÃ , the padding will not affect the computation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[    1.,     1.,     1.],\n",
       "         [    3.,     1.,     0.],\n",
       "         [   10.,     1.,     1.],\n",
       "         [    6.,     0.,     0.]],\n",
       "\n",
       "        [[    3.,     1.,     0.],\n",
       "         [   10.,     1.,     1.],\n",
       "         [    6.,     0.,     0.],\n",
       "         [-1000., -1000., -1000.]],\n",
       "\n",
       "        [[   10.,     1.,     1.],\n",
       "         [-1000., -1000., -1000.],\n",
       "         [-1000., -1000., -1000.],\n",
       "         [-1000., -1000., -1000.]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _padToMatch2d(inputtens, targetshape):\n",
    "    target = torch.full(targetshape, fill_value=-1e3)#torch.zeros(*targetshape)\n",
    "    target[:inputtens.shape[0], :inputtens.shape[1]] = inputtens\n",
    "    return target\n",
    "\n",
    "inputdf = valdf[[tname,dname,*Xnames]].sort_values([dname,tname], ascending=[False,True])\n",
    "\n",
    "tensin = torch.from_numpy(inputdf[[tname,dname,*Xnames]].values)\n",
    "\n",
    "#Get unique event times\n",
    "tensin_events = torch.unique(tensin[tensin[:,1]==1, 0])\n",
    "\n",
    "#For each unique event stack another matrix with event at the top, and all at risk entries below\n",
    "tensor = torch.stack([_padToMatch2d(tensin[tensin[:,0] >= eventtime, :], tensin.shape) for eventtime in tensin_events])\n",
    "\n",
    "#Make sure the top row in each unique event time slice is an event\n",
    "assert all(tensor[:,0,1] == 1)\n",
    "\n",
    "tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need a couple of extra quantities computed from the tensor which are related to how one can deal with _tied event times_ in the Cox model.  \n",
    "\n",
    "We use the _Breslow method_ here to deal with those, which involves summing over the covariates of entries at tied event times, and raising the denominator of the likelihood to the power of the number of tied events."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Cox partial Likelihood is the product over the unique event times, $t_i$, of the ratio of $\\exp(X_i\\beta)$ for the covariates of the subject experiencing an event at that event time, divided by the sum of the equivalent contribution for all the subjects at risk at that event time, $\\sum_{j:\\, t_j \\geq t_i} \\exp(X_j\\beta)$,    \n",
    "$$\\mathcal{L}(\\beta \\;|\\; X) = \\prod_{t_i} \\frac{\\exp(X_i\\beta)}{\\sum_{j:\\, t_j \\geq t_i} \\exp(X_j\\beta)}$$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the presence of tied event times, the Breslow method of dealing with these gives a slightly modified likelihood,\n",
    "$$\\mathcal{L}_B(\\beta \\;|\\; X) = \\prod_{t_i} \\frac{\\exp\\left(\\sum_{k: t_k=t_i} X_k\\beta\\right)}{\\left[\\sum_{j:\\, t_j \\geq t_i} \\exp(X_j\\beta)\\right]^{d_i}}$$\n",
    "where $d_i$ is the number of tied events at time $t_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute some of the ingredients which we will require to compute the Breslow-method Cox likelihood:  \n",
    "- `num_tied` is $d_i$  \n",
    "- `event_tens` is the $\\sum_{k: t_k=t_i} X_k$ which will go into the numerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiecountdf = inputdf.loc[inputdf[dname]==1,:].groupby([tname]).size().reset_index(name='tiecount')\n",
    "num_tied = torch.from_numpy(tiecountdf.tiecount.values).int()\n",
    "\n",
    "#One actually has to sum over the covariates which have a tied event time in the Breslow correction method!\n",
    "#See page 33 here: https://www.math.ucsd.edu/~rxu/math284/slect5.pdf\n",
    "event_tens = torch.stack([tensor[i, :num_tied[i], 2:].sum(dim=0) for i in range(tensor.shape[0])])\n",
    "\n",
    "#Drop time and status columns as no longer required\n",
    "tensor = tensor[:,:,2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the log-likelihood of the (Breslow method) Cox partial likelihood above:  \n",
    "$$\\mathcal{L}_B(\\beta \\;|\\; X) = \\sum_{t_i} \\left[ \\sum_{k: t_k=t_i} X_k\\beta \\;\\;-\\;\\; d_i \\log\\left(\\sum_{j:\\, t_j \\geq t_i} \\exp(X_j\\beta)\\right) \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compute the (Breslow method) Cox partial likelihood, in the cell below:  \n",
    "- `loss_event` gives the numerator: $\\exp\\left(\\sum_{k: t_k=t_i} X_k\\beta\\right)$\n",
    "- `XB` corresponds to $X_j\\beta$ in the second term \n",
    "- `loss_atrisk` then is second term in the likelihood (previously the denominator)\n",
    "\n",
    "\n",
    "This function then returns the negative log-likelihood!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(tensor, event_tens, num_tied, beta):\n",
    "    loss_event = torch.einsum('ik,k->i', event_tens, beta)\n",
    "\n",
    "    XB = torch.einsum('ijk,k->ij', tensor, beta)\n",
    "    loss_atrisk = -num_tied*torch.logsumexp(XB, dim=1)\n",
    "\n",
    "    loss = torch.sum(loss_event + loss_atrisk)\n",
    "\n",
    "    return -loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the optimisation, initialise $\\beta$ values, select optimiser and learning rate, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.1589, grad_fn=<NegBackward>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta = nn.Parameter(torch.zeros(len(Xnames))).float()\n",
    "\n",
    "optimizer = optim.LBFGS([beta], lr=1)\n",
    "\n",
    "\n",
    "def closure():\n",
    "    optimizer.zero_grad()\n",
    "    loss = get_loss(tensor, event_tens, num_tied, beta) #compute the loss\n",
    "    loss.backward() #compute the derivative of the loss\n",
    "    return loss\n",
    "\n",
    "optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.34657338]\n"
     ]
    }
   ],
   "source": [
    "print(beta.detach().numpy()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**And that is indeed the correct value for $\\beta$!** The Maximum Likelihood Estimate for this simple dataset is $\\beta = \\log(2)/2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.34657359027997264"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(2)/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You don't believe me that that is the correct answer? See `notebooks/Validation.ipynb` ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is exactly what is in the `TorchCox()` class in `torchcox/TorchCox.py`, and constitutes the entire fit procedure. You now understand exactly how it works, and it is verifiably correct."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
