{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(8,8)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tyears</th>\n",
       "      <th>d</th>\n",
       "      <th>Karn</th>\n",
       "      <th>Broders</th>\n",
       "      <th>FIGO</th>\n",
       "      <th>Ascites</th>\n",
       "      <th>Diam</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.967124</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>unknown</td>\n",
       "      <td>IV</td>\n",
       "      <td>present</td>\n",
       "      <td>1-2cm</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.665753</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>IV</td>\n",
       "      <td>present</td>\n",
       "      <td>&lt;1cm</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.054794</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>III</td>\n",
       "      <td>present</td>\n",
       "      <td>1-2cm</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.682193</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>III</td>\n",
       "      <td>present</td>\n",
       "      <td>&gt;5cm</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.720549</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>III</td>\n",
       "      <td>unknown</td>\n",
       "      <td>&gt;5cm</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>0.131509</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>III</td>\n",
       "      <td>present</td>\n",
       "      <td>&gt;5cm</td>\n",
       "      <td>354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>4.169864</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>III</td>\n",
       "      <td>unknown</td>\n",
       "      <td>&lt;1cm</td>\n",
       "      <td>355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>4.183561</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>IV</td>\n",
       "      <td>present</td>\n",
       "      <td>&gt;5cm</td>\n",
       "      <td>356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>4.164384</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>III</td>\n",
       "      <td>present</td>\n",
       "      <td>&gt;5cm</td>\n",
       "      <td>357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>1.610960</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>III</td>\n",
       "      <td>absent</td>\n",
       "      <td>micr.</td>\n",
       "      <td>358</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>358 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       tyears  d  Karn  Broders FIGO  Ascites   Diam   id\n",
       "0    0.967124  1     9  unknown   IV  present  1-2cm    1\n",
       "1    2.665753  1    10        4   IV  present   <1cm    2\n",
       "2    1.054794  1    10        3  III  present  1-2cm    3\n",
       "3    0.682193  1     9        2  III  present   >5cm    4\n",
       "4    4.720549  1    10        2  III  unknown   >5cm    5\n",
       "..        ... ..   ...      ...  ...      ...    ...  ...\n",
       "353  0.131509  1     9        3  III  present   >5cm  354\n",
       "354  4.169864  0    10        2  III  unknown   <1cm  355\n",
       "355  4.183561  0     9        2   IV  present   >5cm  356\n",
       "356  4.164384  0    10        4  III  present   >5cm  357\n",
       "357  1.610960  1     9        2  III   absent  micr.  358\n",
       "\n",
       "[358 rows x 8 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/ovarian.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.unique(df.groupby(\"id\").agg({'id':'count'})['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.replace({\"unknown\":pd.NA}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values([\"tyears\",\"id\"], ascending=True, inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAesAAAHYCAYAAABtHcCiAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAABALklEQVR4nO3df0CV5f3/8RcHBSVFfvgL0Ym2cKSV5SlbpSxoYoWgudKwrFlZmZY5M9ICA40AP6RNjXR+3NyX6VqoJDZxZlaraRqxIkqbodHEXyCKKKjn3N8/+HgmSxE4Bzic83z8Jfd9flw353i/ua77ul+Xh2EYhgAAgNMytXYDAABA/SjWAAA4OYo1AABOjmINAICTo1gDAODkKNYAADi5Zi3WxcXFGjdunKKiojRu3Djt27evOd8OAACX1KzFOjExUXFxccrLy1NcXJwSEhKa8+0AAHBJzVasy8rKVFRUpOjoaElSdHS0ioqKVF5e3lxvCQCAS2rXXC9cWlqqHj16yNPTU5Lk6emp7t27q7S0VAEBAZdvmFdwczUNsMvpAx+1dhOaTcdew1q7Cc3KlT87yfU/P1d37sy/L7mv2Yo14Ko4IbZdfHZoq5qtWAcFBenQoUOyWCzy9PSUxWLR4cOHFRQU1FxvCQD1omeNtqrZrlkHBgYqLCxMubm5kqTc3FyFhYU1aAgcAAD8h0dzrrq1d+9excfH68SJE/L19VVqaqr69+/foOdyzRrOypV7Z/TMgNZT3zXrZi3W9qBYAwDcSX3FmgQzAACcHLPBgUZiGLztcuXPTnL9z8+dMQwOAIATaLZh8NTUVEVERGjAgAHas2ePbTuZ4AAAOI5dxToyMlJZWVkKDq7bCyYTHAAAx7HrmrXZbP7RtvOZ4CtXrpRUmwmenJys8vJy7rGGS3Dl655c8wSck8MnmNmbCQ44OwoagJbGrVsAADg5hxfrCzPBJZEJDgCAnRxerMkEBwDAsey6z3revHnavHmzjh49Kn9/f/n5+Wnjxo12ZYKfx33WcFZMMAPQHMgGBwDAydVXrIkbBeA2XHlURGJkxJXRswYAwAmw6hYAAG2YXcPgx44d06xZs/T999/Ly8tLffv2VVJSkgICAlRQUKCEhATV1NQoODhY6enpCgwMdFS7AaDRGAZHW2XXMHhFRYV2796toUOHSqpd2OP48eOaN2+eoqKilJKSIrPZrKVLl6qkpEQpKSkNfm2GweGsXPmEz8keaD3NNsHMz8/PVqglafDgwVq9erUKCwvl7e1tyw4fP368IiMjG1WsAWdFQWu7XPkPLYnvpitz2Gxwq9Wq1atXKyIiQqWlperVq5dtX0BAgKxWqyoqKuTn5+eotwSARqGYoa1yWLFOTk6Wj4+PHnjgAf3tb39z1MsCgMPQs0Zb5ZBinZqaqv379yszM1Mmk0lBQUE6cOCAbX95eblMJhO9agAAmsDuYp2RkaHCwkItW7ZMXl5ekqRBgwapurpau3btktls1po1azRy5Ei7GwsA9qDnibbKrtng3377raKjoxUSEqIOHTpIknr37q0lS5YoPz9fiYmJdW7d6tq1a4Nfm9ngAByNYXA4M7LBAQdy5RM+J3ug9VCsAQBwcsSNAgDQhlGsAQBwciyRCTQS16zbLlf+7CTX//zcmd3FesqUKfrhhx9kMpnk4+Ojl156SWFhYSouLlZ8fLwttSw1NVUhISEOaDLQujghAmhpdk8wq6ysVOfOnSVJW7Zs0ZIlS7Ru3TpNnDhRY8eOVWxsrHJycpSdna1Vq1Y1+HWZYAbA0ehZw5k120IekmyFWpJOnjwpDw8PlZWVqaioSCtXrpQkRUdHKzk5WeXl5QoICLD3LQGgSShmaKsccs16zpw5+vjjj2UYhn73u9+ptLRUPXr0kKenpyTJ09NT3bt3V2lpKcUaAIBGcshs8Pnz52vbtm169tlnlZaW5oiXBAAA/8eht26NHj1aO3bsUM+ePXXo0CFZLBZJksVi0eHDhxUUFOTItwMAwC3YNQxeVVWlEydO2Irw1q1b1aVLFwUGBiosLEy5ubmKjY1Vbm6uwsLCGAKHS3DlSUpc0wWck12zwY8ePaopU6bo9OnTMplM6tKli55//nkNHDhQe/fuVXx8vE6cOCFfX1+lpqaqf//+DX5tZoMDANwJ2eAAINceFZEYGWnryAYHAKANo2cNAIAToGcNAEAb5rBivXjxYg0YMEB79uyRJBUUFCgmJkZRUVGaNGmSysrKHPVWAAC4FYcU66+++koFBQUKDq4durZarXruueeUkJCgvLw8mc1mLViwwBFvBQCA27G7WJ85c0ZJSUmaO3eubVthYaG8vb1lNpslSePHj9emTZvsfSsAANyS3cV60aJFiomJUe/evW3bSktL1atXL9vPAQEBslqtqqiosPftAABwO3YV688//1yFhYWKi4tzVHsAAMB/sStudOfOndq7d68iIyMlSQcPHtQjjzyiBx98UAcOHLA9rry8XCaTSX5+fnY1FgAAd+TQ+6wjIiKUmZmpn/70pxoxYoReffVVmc1mLV26VCUlJUpJSWnwa3GfNQDAndR3n7VD1rP+byaTSWlpaUpMTFRNTY2Cg4OVnp7eHG8FAIDLI8EMAAAn0OI9a8CVufJiECwEATgnetYAADgBssEBAGjD7B4Gj4iIkJeXl7y9vSVJM2fO1LBhw1RQUKCEhIQ6E8wCAwPtbjAAAO7G7mHw87drhYaG2rZZrVZFRUUpJSWFW7cAAGiAFh8GJxscAADHcchs8JkzZ8owDA0ZMkQzZsyoNxucFDMAABrH7p51VlaW3nnnHWVnZ8swDCUlJTmiXQAA4P/YXayDgoIkSV5eXoqLi1N+fr6CgoLIBgcAwEHsGgY/deqULBaLOnfuLMMw9O677yosLEyDBg1SdXW1du3aJbPZrDVr1mjkyJGOajPQqghFAdDS7CrWZWVlmjZtmiwWi6xWq6688kolJiaSDQ4AgAORYAbAbbjyqIjEyEhbV9+tWxRrAACcAAt5AA7kyr0zemaAc6JnDQCAE2jWnnVNTY1eeeUV/eMf/5C3t7cGDx6s5ORkFRcXKz4+3haEkpqaqpCQEHvfDmh19KzbLlf+7CTX//zcmd3FOj09Xd7e3srLy5OHh4eOHj0qSUpMTFRcXJxiY2OVk5OjhIQErVq1yu4GAwDgbuwaBq+qqlJ4eLg++OADXXHFFbbtZWVlioqK0o4dO+Tp6SmLxaKhQ4dq8+bNCggIaNBrMwwOAHAnzTYMXlJSIj8/Py1evFg7duzQFVdcoWeeeUYdOnRQjx495OnpKUny9PRU9+7dVVpa2uBiDQAAatkVN2qxWFRSUqKrr75aa9eu1cyZMzVt2jSdOnXKUe0DAMDt2VWsg4KC1K5dO0VHR0uSrrvuOvn7+6tDhw46dOiQLBaLpNqifvjwYVuOOAAAaDi7hsEDAgI0dOhQffzxx7rttttUXFyssrIyhYSEKCwsTLm5uYqNjVVubq7CwsIYAodLcOUZxcwmBpyT3fdZl5SUaPbs2aqoqFC7du00ffp0hYeHa+/evYqPj9eJEyfk6+ur1NRU9e/fv8GvywQzOCuKNYDmQNwoAABOrr5ibfd61gAAoHlRrAEAcHIUawAAnJxds8F/+OEHPfXUU7afKysrdfLkSX366adkgwMA4CAOnWA2f/58WSwWJSQkaOLEiRo7dqwtGzw7O7tR2eBMMAPgaK48k19iNn9b1yKzwc+cOaPhw4drxYoV6tmzJ9ngAAA0QovMBt+6dat69OihgQMHqrS09JLZ4AAAoHEcVqyzs7M1duxYR70cAAD4Pw4p1ocOHdLOnTs1atQoSbWZ4WSDAwDgGA4p1uvWrVN4eLj8/f0lSYGBgbZscElkgwMAYAeHTDCLiorSnDlzNHz4cNs2ssEBAGg4ssEBAHByZIMDANCGUawBAHByFGsAAJyc3cX6/fff1+jRoxUbG6uYmBht3rxZklRcXKxx48YpKipK48aN0759++x9KwAA3JJdE8wMw9BNN92krKwshYaG6ptvvtH999+vzz77TA8//DDZ4HBJrpwv7erZ0q782Umu//m5uvommNm16pYkmUwmVVZWSqpddat79+46duyYioqKtHLlSklSdHS0kpOTVV5ezr3WAFoNxQxtlV3F2sPDQwsXLtSUKVPk4+OjqqoqLVu2rN5scIo12jpO+G0XPWu0VXZdsz537pzefPNNLV26VO+//77eeOMNTZ8+XadOnXJU+wAAcHt2Feuvv/5ahw8f1pAhQyRJQ4YMUceOHeXt7U02OAAADmLXMHjPnj118OBBfffdd+rfv7/27t2rsrIy9e3b15YNHhsbSzY4AKfAMDHaKrvjRt955x0tX75cHh4ekqSnn35ad9xxB9ngAJwO16zhzMgGBwDAyTXrrVuAu3Hl3hk9M8A50bMGAMAJsOoWAABtGMPgQCMxDA6gpdk9DL5t2zYtWrRI586dU5cuXZSSkqI+ffqouLhY8fHxqqiokJ+fn1JTUxUSEtLg12UYHADgTpptNvjx48c1YsQIrVmzRv369VNOTo7eeecdrVixQhMnTmQhDwAAGqjZZoPv379fXbt2Vb9+/SRJ4eHhmjVrlsrKyljIA4DTceVLGBKXMVyZXcW6X79+Onr0qL744gtde+212rBhgySxkAcAp0QxQ1tlV7Hu3LmzXnvtNaWkpKimpkbDhw+Xr68vC3kAAOBAds8Gv+WWW3TLLbdIko4ePaoVK1YoODjYtpCHp6cnC3kAAGAHu++zPnLkiCTJarUqIyND48ePV3BwsG0hD0ks5AEAgB3svnVrzpw5ys/P19mzZ3Xrrbdq9uzZ8vb2ZiEPAAAagYU8AABwcsSNAgDQhhE3CjSSK9+r6+q3NrnyZye5/ufnzhgGBwDACdg1DJ6amqqIiAgNGDBAe/bssW0vLi7WuHHjFBUVpXHjxmnfvn0N2gcAABrnssU6MjJSWVlZCg6u29NNTExUXFyc8vLyFBcXp4SEhAbtAwAAjXPZYm02m38UZnI++zs6OlpSbfZ3UVGRysvL690HAAAar0kTzOrL/jYMg1xwAAAciFu3AABwck3qWQcFBV0y+9swDHLBAQBwoCb1rAMDAy+Z/V3fPgAA0HiXvc963rx52rx5s44ePSp/f3/5+flp48aN9WZ/25sLLnGfNZyXKwdrEKoBtB6ywQEAcHJkgwMA0IZRrAEAcHIUawAAnNxlb91KTU1VXl6e/v3vf2vDhg0KDQ2td7tUmw0eHx+viooK+fn5KTU1VSEhIc12EEBLYoIZgJZ22WIdGRmpiRMnasKECQ3aLv0nGzw2NlY5OTlKSEjQqlWrHNdqoBVR0AC0tCZlg9e3nWxwAAAcy+HXrOvLDQcAAI3HBDMAAJxck7LB61NfbjjgCphgBqClObxYX5gNHhsbSzY4XA4Fre1y5T+0JL6brqzJ2eCX2i6RDQ4AQGORDQ4AgJMjGxwAgDaMYg0AgJOjWAMA4OQaVKxTU1MVERGhAQMGaM+ePZKkY8eO6bHHHlNUVJRGjRqlqVOn1kkpKygoUExMjKKiojRp0iSVlZU1zxEAAODiGlSsIyMjlZWVpeDg/0z68vDw0KOPPqq8vDxt2LBBffr00YIFCyRJVqtVzz33nBISEpSXlyez2WzbBwAAGqdBxfpiOeB+fn4aOnSo7efBgwfrwIEDkqTCwkJ5e3vLbDZLksaPH69NmzY5qs0AALgVh1yztlqtWr16tSIiIiTV5oP36tXLtj8gIEBWq1UVFRWOeDsAANyKQxLMkpOT5ePjowceeMARLwc4NVdOwXL1BCxX/uwk1//83JndxTo1NVX79+9XZmamTKbajnpQUJBtSFySysvLZTKZ5OfnZ+/bAa2OEyKAlmbXMHhGRoYKCwu1ZMkSeXl52bYPGjRI1dXV2rVrlyRpzZo1GjlypH0tBQDATTUobvRiOeALFy5UdHS0QkJC1KFDB0lS7969tWTJEklSfn6+EhMTVVNTo+DgYKWnp6tr164NbhhxowAcjWFwODOywQEAcHJkgwMA0IZRrAEAcHIUawAAnNxli/XFcsElacqUKYqJidHo0aMVFxenr7/+2ravuLhY48aNU1RUlMaNG6d9+/Y1S+MBAHAHl51gtmvXLgUHB2vChAnKzMxUaGioJKmyslKdO3eWJG3ZskVLlizRunXrJEkTJ07U2LFjFRsbq5ycHGVnZ2vVqlWNahgTzAAA7sSuCWYXywWXZCvUknTy5El5eHhIksrKylRUVKTo6GhJUnR0tIqKiuqsyAUAABrOrgSzOXPm6OOPP5ZhGPrd734nqTYXvEePHvL09JQkeXp6qnv37iotLVVAQID9LQZamSvfq+vq9+m68mcnuf7n587sKtbz58+XJK1fv15paWlavny5QxoFODNOiG0Xnx3aKofMBh89erR27NihY8eOKSgoSIcOHZLFYpEkWSwWHT58+KJD6QAA4PKa1LOuqqrSiRMnbAV469at6tKli/z8/OTh4aGwsDDl5uYqNjZWubm5CgsLYwgcQKtjGBxt1WVng18sF/wPf/iDpkyZotOnT8tkMqlLly56/vnnNXDgQEnS3r17FR8frxMnTsjX11epqanq379/oxrGbHAAjkaxhjMjGxwAACdXX7G2ez1rAGgr6FmjraJnDQCAE2DVLQAA2rAGFetL5YOft3jx4h/tKygoUExMjKKiojRp0iSVlZU5rtUAALiRBhXryMhIZWVlKTj4x0PTX331lQoKCurss1qteu6555SQkKC8vDyZzWYtWLDAca0GAMCNNKhYXyof/MyZM0pKStLcuXPrbC8sLJS3t7fMZrMkafz48dq0aZP9rQUAwA3Zdc160aJFiomJUe/evetsLy0tVa9evWw/BwQEyGq1qqKiwp63AwDALTW5WH/++ecqLCxUXFycI9sDAAD+S5Pvs965c6f27t2ryMhISdLBgwf1yCOPKCUlRUFBQTpw4IDtseXl5TKZTPLz87O7wQAAuJsmF+vJkydr8uTJtp8jIiKUmZmp0NBQWa1WVVdXa9euXTKbzVqzZo1GjhzpkAYDAOBuGlSsL8wH//Wvfy0/Pz9t3Ljxko83mUxKS0tTYmKiampqFBwcrPT0dIc1GgAAd0KCGQAAToAEMwAA2jAW8gAayZUXg2AhCMA5MQwOAIATsHuJzNTUVOXl5enf//63NmzYoNDQUEm1M8C9vLzk7e0tSZo5c6aGDav9y7ygoEAJCQl1JpgFBgbaeywAALgdu7PBX3/9deXk5CgnJ8dWqMkGBwDAcezKBr8UssEBAHAcuyeYzZw5U4ZhaMiQIZoxY4Z8fX3rzQYnxQwAgMax69atrKwsvfPOO8rOzpZhGEpKSnJUuwAAwP+xq1ifHxr38vJSXFyc8vPzbdvJBgcAwDGaXKxPnTqlyspKSZJhGHr33XcVFhYmSRo0aJAtG1wS2eAAANihQfdZX5gN7u/vLz8/P2VmZmratGmyWCyyWq268sor9eKLL6p79+6SpPz8/B9lg3ft2rXBDeM+awCAO6nvPmtCUQAAcAJ2h6IAgCtw5ahYibhYV0bPGgAAJ8CqWwAAtGEUawAAnNxlr1lfahGPmpoavfLKK/rHP/4hb29vDR48WMnJyZKk4uJixcfH2xLLUlNTFRIS0qwHArQUV77uyTVPwDldtlhHRkZq4sSJmjBhQp3t6enp8vb2Vl5enjw8PHT06FHbvsTERMXFxSk2NlY5OTlKSEjQqlWrHN96oBVQ0NouV/5DS+K76couW6zPL8ZxoaqqKq1fv14ffPCBPDw8JMl2D3VZWZmKioq0cuVKSVJ0dLSSk5NVXl6ugIAAR7YdABqFYoa2qknXrEtKSuTn56fFixfrnnvu0YMPPmhLKystLVWPHj3k6ekpSfL09FT37t1VWlrquFYDAOBGmlSsLRaLSkpKdPXVV2vt2rWaOXOmpk2bppMnTzq6fQAAuL0mFeugoCC1a9dO0dHRkqTrrrtO/v7+Ki4uVlBQkA4dOiSLxSKptrAfPny4UethAwCA/2hSgllAQICGDh2qjz/+WLfddpuKi4tVVlamvn37ytfXV2FhYcrNzVVsbKxyc3MVFhbG9Wq4DFeepMQ1XcA5XTbB7GKLeGzcuFElJSWaPXu2Kioq1K5dO02fPl3h4eGSpL179yo+Pl4nTpyQr6+vUlNT1b9//0Y1jAQzAIA7YSEPwIHoWQNoDsSNAgDQhtGzBgDACdCzBgCgDWtSNvgPP/ygp556yvaYyspKnTx5Up9++qkkssEBAHCkJmWD9+7dWzk5Obaf58+fb7uvWiIbHAAAR7rsMLjZbK430OTMmTPasGGDxo4dK+k/2eDnA1Oio6NVVFSk8vJyBzUZAAD3Yvc1661bt6pHjx4aOHCgJLLBAQBwtCYlmF0oOzvb1qsGAGfmyvfIS9wn78rsKtaHDh3Szp07lZaWZtt2YTa4p6cn2eBwOa58wnf1k72rHx9cl13Fet26dQoPD5e/v79tW2BgINngcGmc8AG0tCZng0tSVFSU5syZo+HDh9d5DtngAAA0DtngAAA4ORLMAABowyjWAAA4OYo1AABOrknZ4JL0/vvva9GiRTIMQ4ZhaOrUqRoxYoQkssHh2rh1q+1y5c9Ocv3Pz51ddoLZrl27FBwcrAkTJigzM1OhoaEyDEM33XSTsrKyFBoaqm+++Ub333+/PvvsM5lMJk2cOFFjx461ZYNnZ2c3OhucCWYAAHdi1wSzS2WDm0wmVVZWSqpddat79+4ymUxkgwMA4GBNCkXx8PDQwoULNWXKFPn4+KiqqkrLli2TVH82OMEoAAA0XpOK9blz5/Tmm29q6dKlGjJkiD777DNNnz7dFpYCuDJXvu7JNU/AOTWpWH/99dc6fPiwhgwZIkkaMmSIOnbsqL179yo4OJhscLg0ChqAltakW7d69uypgwcP6rvvvpNUGy9aVlamn/zkJ3WywSWRDQ4AgJ2anA3+zjvvaPny5fLw8JAkPf3007rjjjskkQ0OAEBjkQ0OAICTIxscAIA2jGINAICTo1gDAODkLlusU1NTFRERoQEDBmjPnj227du2bdOYMWM0atQoPfDAAyopKbHtKy4u1rhx4xQVFaVx48Zp3759zdJ4AADcwWWLdWRkpLKyshQc/J8JX8ePH9fzzz+vjIwMbdiwQffee6/mzp1r25+YmKi4uDjl5eUpLi5OCQkJzdJ4AADcQZOywffv36+uXbuqX79+kqTw8HD9/e9/V3l5OdngAAA4WJOuWffr109Hjx7VF198IUnasGGDpNpc8PqywQEAQOM1KW60c+fOeu2115SSkqKamhoNHz5cvr6+8vT01Llz5xzdRgAA3FqTirUk3XLLLbrlllskSUePHtWKFSv0k5/8RKdPnyYbHIBTcuVFWCRy611Zk4v1kSNH1K1bN1mtVmVkZGj8+PHy8fGRj4+PLRs8NjaWbHC4HFc+4bv6yd7Vjw+uq8nZ4HPmzFF+fr7Onj2rW2+9VbNnz5a3t7ckssHh2ijWAJoD2eCAA1GsATQHssEBAGjD6FkDAOAE6utZN3mCGeCuGAYH0NIu27M+duyYZs2ape+//15eXl7q27evkpKSFBAQoIKCAiUkJKimpkbBwcFKT09XYGCgJNW7ryHoWQMA3Ild16w9PDz06KOPKi8vTxs2bFCfPn20YMECWa1WPffcc0pISFBeXp7MZrMWLFggSfXuAwAAjXPZYXA/Pz8NHTrU9vPgwYO1evVqFRYWytvbW2azWZI0fvx4RUZGKiUlpd59ANBaXPkShsRlDFfWqGvWVqtVq1evVkREhEpLS9WrVy/bvoCAAFmtVlVUVNS7z8/Pz2GNB4DGoJihrWpUsU5OTpaPj48eeOAB/e1vf2uuNgFAs6BnjbaqwcU6NTVV+/fvV2Zmpkwmk4KCgnTgwAHb/vLycplMJvn5+dW7DwBaC8UMbVWDQlEyMjJUWFioJUuWyMvLS5I0aNAgVVdXa9euXZKkNWvWaOTIkZfdBwAAGueyt259++23io6OVkhIiDp06CBJ6t27t5YsWaL8/HwlJibWuT2ra9euklTvvobg1i0AgDshGxwAACdHNjgAAG0YcaNAI7nyjGImYAHOiWFwAACcgF3D4MeOHdNjjz2mqKgojRo1SlOnTlV5ebkk6Te/+Y1uu+02DRgwQFVVVXWeV1BQoJiYGEVFRWnSpEkqKyuz8zAAAHBPTc4Gl6Rf/epXysnJ+dFzyAYHAMBxLlusL5YNfj7w5Oc///lFV9K6WDb4pk2bHNVmAADcSqNmg1+YDV6f+rLBAQBA4zSqWF+YDQ4AAFpGk7PB60M2OAAAjtPkbPD6kA0OAIDj2JUNPnXqVH3xxRc6dOiQunfvrtDQUK1YsUIS2eAAADQG2eAAADg5ssEBAGjDKNYAADg5ijUAAE7usrduHTt2TLNmzdL3338vLy8v9e3bV0lJSTp+/LgSEhJ05MgRtWvXTtdcc40SExNtk9C2bt2qtLQ0WSwWDRw4UCkpKerYsWOzHxAAXIorr5gmsWqaK7vsBLOKigrt3r3bFjmampqq48ePa8qUKTpx4oSuvvpqWa1WzZgxQ1dddZWeeuopVVVVacSIEcrKylJISIjmzJmjoKAgTZ06tcENY4IZAMCd1DfB7LI964tlg69evVq9e/e2bTOZTLr22mu1d+9eSdKHH36oQYMGKSQkRFJtNnh8fHyjijXgrFy5d0bPDHBODU4wky6dDV5dXa3s7GzNmDFD0o+zwXv16qXS0lIHNBdofRQ0AC3N7mzwc+fO6dlnn9XNN9+syMhIhzcQAAB31+BifT4bfOHChbZscIvFopkzZ6pLly568cUXbY/972zwAwcOKCgoyIHNBgDAfTQ5G9xqtSo+Pl6enp6aP3++PDw8bI8fNmyYvvzyS+3bt09SbTb4nXfe6fjWAwDgBpqcDX7vvffq8ccfV2hoqK2nfcMNNygxMVGStGXLFqWnp8tqtSosLEyvvvqqfHx8GtwwZoMDANwJ2eAAADg5ssEBAGjDKNYAADg5ijUAAE6OYg0AgJNr8kIefn5+uv/++3X69GlJUrdu3fTyyy/bYkgLCgqUkJCgmpoaBQcHKz09XYGBgc17NAAAuKAmL+TxyiuvqLKyUp07d5Yk/eEPf9DOnTu1ePFiWa1WRUVFKSUlRWazWUuXLlVJSYlSUlIa3DBmgwMA3Ilds8EvtpDH+XSy84Vakk6ePGm737qwsFDe3t4ym82Sahfy2LRpU9NaDwCAm7N7IY/HHntMRUVF8vf314oVKyT9eCGPgIAAWa1WVVRUyM/PzzEtBwDATdi9kMfy5cv10Ucf6e6779Ybb7zh8AYCAODu7FrIw/YiJpN+9atfKScnR9KPF/IoLy+XyWSiVw0AQBM0eSGP8vJylZeX2x6zadMmDRgwQJI0aNAgVVdXa9euXZJqF/IYOXKko9sOAIBbaPJCHk8//bReeOEFnT17VpIUHBysOXPmqE+fPpKk/Px8JSYm1rl1q2vXrg1uGLPBAQDuhIU8AABwcizkAQBAG0axBgDAyVGsAQBwcpct1seOHdNjjz2mqKgojRo1SlOnTq0zC1ySXnjhBQ0YMEBVVVW2bVu3btXIkSP1y1/+UtOnT7dliAMAgMaxKxtcqi3KW7ZsUXZ2tvLz83XFFVeoqqpKI0aMUFZWlkJCQjRnzhwFBQVp6tSpDW4YE8wAONrpAx+1dhOaVcdew1q7CbBDs2WDHzt2TIsXL9YLL7xQ5zkffvihBg0apJCQEEm12eB//etfm9J2AADcXqOuWf93NnhSUpKefvrpOgt6SD/OBu/Vq5dKS0sd0FwAANxPoxbyuDAb/N1331X79u31i1/8opmaBgCOxTAx2qoGF+vz2eCZmZkymUz69NNPtX379jorcEVHR2v58uUKCgrSjh07bNsPHDigoKAgx7YcaCWufN2TYgY4pwYlmGVkZOjzzz/XsmXL1LFjx4s+ZsCAAbYJZidPntSIESP0pz/9iQlmcDkUawDNob4JZpftWX/77bd68803FRISovHjx0uqzQZfsmTJJZ/TqVMnJSUl6fHHH5fValVYWJjmzJnThKYDAACywYFGomcNoDmwkAcAAE6OhTwAAGjDGnXrFgCGwQG0vMsOgx87dkyzZs3S999/Ly8vL/Xt21dJSUkKCAjQgAEDFBoaKpOptoOelpamAQMGSKqNIU1LS5PFYtHAgQOVkpJyyZnkF8MwOADAndh1zbq+bPALb9e6ENngAJyRK4+KSIyMtHXNlg1+KWSDAwDgOI26Zv3f2eCS9OCDD8pisWj48OGaNm2avLy8yAYH4JToeaKtatRs8AuzwSVp27ZtWrt2rbKysvSvf/2r3qAUAADQNA0u1uezwRcuXGibUHY+77tTp0669957lZ+fb9t+4VA52eAAADRdg4bBMzIyVFhYqGXLlsnLy0uSdPz4cXl7e6tDhw46d+6c8vLyFBYWJkkaNmyYkpOTtW/fPoWEhGjNmjW68847m+8ogBbkypOUGCYGnNNlZ4N/++23io6OVkhIiDp06CCpNhv80UcfVUJCgjw8PHTu3Dldf/31mj17tm1m+JYtW5Senm7LBn/11Vfl4+PT4IYxGxwA4E6IGwUAwMnZteoWALgKV76EIXEZw5XRswYayZVP+JzsgdbDMDgAAE7OrmHw+rLBKyoqlJSUpK+++krt2rXTnXfeaYsULSgoUEJCgmpqahQcHKz09HQFBgY67qgAAHATdmWDP/HEE7r55pv18MMPS5KOHDmibt26yWq1KioqSikpKTKbzVq6dKlKSkqUkpLS4IbRswYAuJNmyQbft2+f9uzZo4ceesi2r1u3bpKkwsJCeXt7y2w2S6rNBt+0aVOTDwAAAHfWqLjRC7PB//Wvf6lHjx6aM2eOxowZo8cee0zffvutJP0oGzwgIEBWq1UVFRUObTwAAO6gydngVqtV//znP3XPPfdo3bp1uvfee/Xkk082VzsBAHBbTc4GDwoKUlBQkG2oe8SIETpy5IjKy8t/lA1eXl4uk8kkPz8/hx8AAACurkHF+nw2+JIlS2zZ4IMGDZKPj49t6Hvnzp3q0qWL/P39NWjQIFVXV2vXrl2SpDVr1mjkyJHNdAgAALi2JmeDL1myRF9++aVefvllnTlzRh07dtScOXN07bXXSpLy8/OVmJhY59atrl27NrhhzAYHALgTQlEAAHBydt26BQAAWhfFGgAAJ0exBgDAyTU5G3zfvn16+eWXbY8rKytTt27dtG7dOklkgwMA4Ch2ZYNfaMqUKRoyZIgeeeQRssEBAGikZskGv1BZWZk+/vhjxcbGSiIbHAAAR2pyNviF1q9fr1tvvdV2HzXZ4AAAOM5lr1lf6MJs8AutXbtWM2bMcGjDAGd1+sBHrd2EZtOx17DWbkKzcuXPTnL9z8+dNbhYn88Gz8zMlMn0nw55QUGBjh8/rvDwcNs2ssHhyjghtl18dmirmpwNfl52drZiYmLUrt1/6j7Z4AAAOI5d2eDV1dW69dZb9dZbb+nKK6+s8zyywQEAaDiywQFAXLOGc6NYAwDg5Oor1o2aDQ7AtXtn9MwA50TPGgAAJ2BXz/pS2eABAQF6++239Yc//EEmk0menp6aPXu2LbWMbHAAAByjydngzz33nCIjI7V582Z17dpV7733nv7nf/5H7777LtngAAA0kl0964tlg69evVqGYcgwDFVVValr166qrKxUz549JV08GzwyMrJRxRoAHM2V5xtIzDlwZY2aYHZhNnhAQICSkpI0ZswY+fr6ymq16o9//KOk+rPBSTED0FooZmirGrWQx4XZ4CdPnlRWVpbefvttbdu2TfHx8Zo6daqcdL4aAABtVoOL9fls8IULF8pkMunvf/+7OnfurP79+0uS7rrrLn3//fc6duwY2eAAADhQk7PBe/furaKiIpWVlUmStm/frk6dOsnf359scAAAHMiubPCVK1fqrbfeUvv27eXl5aX4+HjbpDKywQEAaDjiRgEAcHL1FetGTTADAAAtj2INAICTo1gDAODk7MoGz87O1u9//3tZrVb16dNHr776qu32LLLBAQBwjCZngz/yyCN6+OGHlZOTo4CAAC1dulQHDx5UUlIS2eAAADSSXRPMLpYNfuDAAe3Zs0dhYWEKCAiQJIWHh2vDhg2SLp4NvmnTJrsOAgAAd9Woa9YXZoP/7Gc/05dffqmSkhIZhqHc3FydOnVKFRUV9WaDAwCAxmnUQh4XZoObTCa9+OKLevbZZ+Xh4aHIyMjaF2zXqJcEAACX0eDKej4bPDMzUyZTbYf87rvv1t133y1J+uKLL/SnP/1JnTp1IhscAAAHanI2uCQdOXJEklRTU6PXX39dkyZNkiSywQEAcCC7ssEfffRRHThwQGfPntVdd92lZ555xtbrJhscAICGIxsccKDTBz5q7SY0m469hrV2EwC3RbEGAMDJsZAHAABtGMUaAAAnR7EGAMDJNeg+6ylTpuiHH36QyWSSj4+PXnrpJYWFham4uFjx8fGqqKiQn5+fUlNTFRISIkn17gMAAA3XoAlmlZWV6ty5syRpy5YtWrJkidatW6eJEydq7Nixio2NVU5OjrKzs7Vq1SpJqndfQzDBDADgTuyeYHa+UEvSyZMn5eHhobKyMhUVFSk6OlqSFB0draKiIpWXl9e7DwAANE6D40bnzJmjjz/+WIZh6He/+51KS0vVo0cPeXp6SpI8PT3VvXt3lZaWyjCMS+47v0oXAABomAZPMJs/f762bdumZ599Vmlpac3ZJgAAcIFGzwYfPXq0duzYoZ49e+rQoUOyWCySJIvFosOHDysoKEhBQUGX3AcAABrnssW6qqpKpaWltp+3bt2qLl26KDAwUGFhYcrNzZUk5ebmKiwsTAEBAfXuAwAAjXPZ2eBHjx7VlClTdPr0aZlMJnXp0kXPP/+8Bg4cqL179yo+Pl4nTpyQr6+vUlNT1b9/f0mqd19DMBscAOBOyAYHAMDJkQ0OAEAbRrEGAMDJNfg+awBo61x5LXKJ9chdWYOuWV8qGzw1NVV5eXn697//rQ0bNig0NNT2HHuzwblmDQBwJ3ZPMLtUNviuXbsUHBysCRMmKDMzs06xJhscAICGa5ZscEkym80XDTohGxwAAMdpcjZ4ferLDScYBQCAxmlwsZ4/f74kaf369UpLS9Py5cubrVEA0ByYYIa2qsnZ4MeOHbvkY8gGBwDAcS7bs66qqtKJEydshfZ8Nrifn98ln3NhNnhsbCzZ4HAprtw7c/WemasfH1yXXdng8+bN0+bNm3X06FH5+/vLz89PGzdulEQ2OFwXxRpAcyAbHHAgijWA5kCxBgDAydVXrIkbBeA2XHlURGJkxJXRswYAwAnY3bO+WDZ4z549NWvWLH3//ffy8vJS3759lZSUZJvxXVBQoISEBNXU1Cg4OFjp6ekKDAx0zBEBAOBGmpwNvnLlSu3evVtDhw6VJKWmpur48eN65ZVXZLVaFRUVpZSUFJnNZi1dulQlJSVKSUlpcMPoWQMA3EmzZIP7+fnZCrUkDR48WAcOHJAkFRYWytvbW2azWZI0fvx4bdq0qUmNBwDA3TkkG9xqtWr16tWKiIiQVJsN3qtXL9v+gIAAWa1W23KZAACg4RocNzp//nxt27ZNzz77rNLS0ursS05Olo+Pjx544AGHNxAAAHdndzZ4amqq9u/fr4ULF8pkqn25oKAg25C4JJWXl8tkMtGrBgCgCezKBs/IyFBhYaGWLVsmLy8v23MGDRqk6upq7dq1S2azWWvWrNHIkSOb7ygAoAG4zxptVZOzwb28vBQdHa2QkBB16NBBktS7d28tWbJEkpSfn6/ExMQ6t2517dq1wQ1jNjgAR6NYw5kRNwoAgJMjbhRwIFfundEzA5wTPWsAAJyA3aEoAACg9TQ5GzwsLOyS2yWpuLhY8fHxtiCU1NRUhYSENOexAC2CYXAALa3J2eDr1q275HZJmjhxosaOHavY2Fjl5OQoOztbq1atanDDGAaHs6JYA2gOdk8wu1g2eH3by8rKVFRUpJUrV0qSoqOjlZycrPLyctuqXEBbRUED0NLszga/2PbS0lL16NFDnp6ekiRPT091795dpaWlFGsAABqpwcV6/vz5kqT169crLS1Ny5cvr3c7ADgbV76EITHq48qadOvWtddeqw8++ED+/v4X3X5+PesdO3bI09NTFotFQ4cO1ebNmxvcs+aaNQDAndh1zfpS2eDt27dXaWnpRTPDPTw8FBYWptzcXMXGxio3N1dhYWEMgcMluHLvjJ4Z4JwuW6xPnz6tZ555pk42eGZmpqqrqy+6/fwks7lz5yo+Pl5Lly6Vr6+vUlNTm/1gAABwRSSYAQDgBEgwAwCgDaNYAwDg5CjWAAA4uQYV6ylTpigmJkajR49WXFycvv766zr7Fy9erAEDBmjPnj22bQUFBYqJiVFUVJQmTZqksrIyx7YcAAA30aBinZqaqnfeeUfr16/XpEmTNHv2bNu+r776SgUFBQoO/s+EMKvVqueee04JCQnKy8uT2WzWggULHN96AADcQIOK9aUywM+cOaOkpCTNnTu3zuMLCwvl7e0ts9ksSRo/frw2bdrkoCYDAOBe7MoGX7RokWJiYtS7d+86jy0tLVWvXr1sPwcEBMhqtdqWywQAAA3X5GzwKVOmqLCwUDNnzmy2xgEAgCbMBh89erR27Nih7du3a+/evYqMjFRERIQOHjyoRx55RH//+98VFBSkAwcO2J5TXl4uk8lErxoAgCZocjb4E088oSeffNL2uIiICGVmZio0NFRWq1XV1dXatWuXzGaz1qxZo5EjRzbfUQAA4MKanA1+fpLZxZhMJqWlpSkxMVE1NTUKDg5Wenq6QxsOAIC7IBscgNtw5RXTJFZNa+vIBgcAoA2jZw0AgBOgZw0AQBvWoPusp0yZoh9++EEmk0k+Pj566aWXFBYWpoiICHl5ecnb21uSNHPmTA0bVnvNpKCgQAkJCXUmmAUGBjbfkQAA4KIaNAxeWVlpixzdsmWLlixZonXr1tW5XetCVqtVUVFRSklJkdls1tKlS1VSUqKUlJQGN4xhcACOxgQzOLP6hsEb1LO+VDb4pVwsGzwyMrJRxRoAHI1ihrbKrmxwqXbo2zAMDRkyRDNmzJCvry/Z4AAAOFCDJ5jNnz9f27Zt07PPPqu0tDRJUlZWlt555x1lZ2fLMAwlJSU1W0MBAHBXTc4GP3bsmC2C1MvLS3FxccrPz5ckssEBAHCgyxbrqqoqlZaW2n4+nw3u7e2tyspKSZJhGHr33XcVFhYmSRo0aJAtG1wS2eAAANihydngZWVlmjZtmiwWi6xWq6688kolJiZKIhscAABHIsEMAAAnQIIZAABtGMUaAAAnR7EGAMDJUawBAHByTjvBDAAA1KJnDQCAk6NYAwDg5CjWAAA4OYo1AABOjmINAICTo1gDAODkKNYAADg5ijUAAE6OYg0AgJO77HrWbcFvf/tbPf744/Ly8mrtpji9AQMGKD8/X1dccUVrN6VeERERyszMVGhoqE6fPq2pU6eqe/fumjdvnjw9PVu7efWKiIiQl5eXvL29VVNTI7PZrMTERLVv377Jr7l27Vpt27ZNr7/+ugNb2jwuPH5JGjp0qDp37qxTp07p+eeflyTt27dPGRkZKiwslK+vr6xWq8LDwzV9+nR5enrKarVq2bJlWrdunUwmkzw8PPTwww/rvvvua7HjOH78uIYNG6b77rtPL774YpNeY9GiRbrqqqt01113aceOHTp79qxuu+02B7fUMc5/bl5eXjp9+rR++tOf6rHHHtMNN9yg1atXq6amRg8//HBrN7NZxcbG6s9//rM6dOjwo30XnpNag0sU68WLF2vSpEnNXqytVqs8PDzk4eHRrO+D/zhx4oQmT56sa665RrNnz27w795isbRqUX/99dcVGhoqi8WiCRMm6G9/+5vuuusu2/6W+C615vf1/PGf99vf/tb278OHD2vChAn6zW9+Y/vj4+TJk1q+fLnOnDmjjh076o033tD27dv11ltvqUuXLiotLdWjjz4qb29vxcbGtsgx5Obm6rrrrtPGjRs1a9asJp1fnnnmGdu/P/30U506dcppi7VU93PbvHmzJk+erBUrVuj+++9v5Za1jJycnNZuwiW1+WL98ssvS5LGjx+vM2fOqLS0VDt37rT9Vf/EE0/o7rvv1qhRo/TBBx/ojTfe0JkzZ9S+fXu98MILGjx4sI4cOaIZM2aoqqpKNTU1Cg8P16xZsyTVnmS+/fZbnTx5UgcOHNDq1au1aNEibd++XV5eXvLx8dGaNWta7fgvZ/PmzcrIyJC3t7dGjBjR2s1plLKyMs2aNUsRERF6+umnJUm/+c1vVFxcrLNnz+onP/mJXnnlFXXp0kU7duzQvHnzNGjQIBUVFWn69OlKTk5WbGysPvnkEx05ckSTJk3SAw880KLHUFNTo5qaGvn6+v7ou/TnP/9Z77//vlasWCFJ+slPfqKkpCQFBgbqzJkzmjdvnrZv3y5/f3+FhYXVed1ly5Zp8+bNslgs6tGjh5KTk9WtW7c28X3NysrS0KFDdc8999i2derUSc8++6yk2t/ZsmXLtHbtWnXp0kWSFBQUpFmzZtk+05aQnZ2t5557Tm+++abee+893XnnndqyZYsWLVokk8kki8Wil156SUOHDtWhQ4c0b9487du3T5IUHR2txx9/XPHx8Ro0aJBuvPFGrVmzRlarVZ988onuvvtuTZ48+ZLnpO+++04vvPCCTp8+LavVqjFjxuiRRx5pkeM+b8SIEfriiy+0YsUKXXXVVbaRkd27d+vll1/W6dOnVVNTo/vuu8/W446Pj5eXl5f27dunkpIS/fKXv9Ttt9+u3/72tzp48KAeeughPfTQQy16HI1x4cjjrl27bPXlxhtvVKsvo2G4gNDQUOPkyZOGYRjG9OnTjbVr1xqGYRglJSXGrbfeatTU1Bj79+837rvvPqOystIwDMPYs2ePER4ebhiGYVRXV9uef+bMGePBBx80PvjgA8MwDOP11183wsPDjbKyMsMwDOOrr74yRo4caVgsFsMwDKOioqLFjrOxjhw5Ytx0003G3r17DcMwjGXLltX5XTmz22+/3bjpppuMhQsX1tl+/nMwDMPIyMgw0tPTDcMwjO3btxs/+9nPjPz8/Dqv8eqrrxqGUftdGDx4cIsc++23325ERUUZMTExxuDBg42pU6cahvHj79Lu3buNW2+91Th06JBhGIbx2muvGc8884xhGIaxatUq49e//rVx5swZ49SpU8aYMWOMadOmGYZhGOvXrzdefPFF23cwKyvLmDFjxkXfozW+rxcef0xMjPHhhx8ar7/+uu2zePTRR42VK1de8vm7d+82hgwZ8qPtFRUVLfb9/frrr43bb7/dsFqtRk5OjvHII48YhmEYo0aNsn3Hzp07ZzufPPDAA8by5cttzz//+3/++eeNP/7xj4ZhGHV+B4Zh1HtOSk5ONjIzM22PbanPbffu3XW2bd682bjzzjvrtL2ystKoqakxDMMwTp48adx5553Gv/71L8Mwao93/PjxRk1NjXHq1Cnj5ptvNuLj4w2LxWIcPHiwxf4PNtX571dNTY1x2223Gdu3bzcMwzA2btxohIaG/uj305LafM/6vz344INKSUnRmDFjtGbNGo0dO1ZeXl766KOP9P3332vChAm2x547d05Hjx6Vj4+P0tLS9Pnnn8swDB09elTffPONhg8fLkkaPny4AgICJEl9+vTRuXPnNGfOHA0dOlS33357qxxnQ/zzn//U1Vdfrf79+0uSxo0bpwULFrRyqxouPDxc7777rsaPH68ePXpIqh2m2rBhg86ePatTp04pJCTE9vi+ffvq+uuvr/Ma54eee/fuLV9fXx08eFBXXnlls7f9/HBiTU2Npk2bpt///veS6n6XduzYofDwcHXv3l1S7ejQ+V7jjh07NHr0aLVv317t27dXTEyM8vPzJUlbt25VYWGhxowZI6l2yL9Tp06293aG7+t/D4MXFBRc8rHLli3Txo0bVVFRoddee+2y8ymMFujhvP3224qNjZWHh4dGjBihefPm6dChQ7r55puVkpKiESNGaPjw4QoNDVVVVZU+//xzrVy50vb887//+tR3TrrxxhuVnp6u06dPa+jQobr55pub5Tgv52K/6+rqas2dO1e7d++Wh4eHDh8+rG+++cb2/+qOO+6wXTLo16+fwsPDZTKZ1KNHjxb9P2iP7777Th07dtTQoUMl1Z5HEhISWrVNLlesb7jhBlksFn322Wdat26d3n77bdu+YcOGKS0t7UfPWbJkiU6cOKG//OUv8vb21ksvvaSamhrb/gtPHp07d9bGjRu1Y8cOffLJJ1qwYIHWrVunbt26Ne+BuaFHH31U77//viZOnKhVq1appKREq1ev1po1axQQEKANGzborbfesj3ex8fnR69x/nKIJHl6espisbRI2y98/1/84hfatm2brrnmGodM7DMMQ08++aR+9atfXXS/s39fr776an355Ze2nydPnqzJkyfrnnvu0dmzZxUSEqKzZ89q7969dU7qBQUF6t27d50/TJrDmTNnlJubKy8vL9s1zLNnz2rt2rWaPXu2du/ere3bt+uZZ57Rr3/9a919991Nfq9LnZOioqI0ePBgffzxx1q+fLmys7Nb5Q/tL7/8UldddVWdbRkZGerWrZteffVVtWvXTpMmTapzvvzv/3Ot/X/QUVp7rpJL3Lp1xRVX6OTJk7afH3zwQc2YMUPXX3+9goKCJEm33nqrPvroI3377be2x33xxReSpMrKSnXr1k3e3t46dOiQ3nvvvUu+V3l5uU6fPq1hw4Zp5syZ6ty5s0pKSprpyOwzePBgFRUV2a6j/eUvf2ndBjXB448/rjFjxmjixIkqKSlRp06d5OfnpzNnzig7O7u1m3dZVqtVO3furDMCcN7QoUP1wQcf6MiRI5Kkt956S7fccosk6eabb1ZOTo7OnTun6upq5ebm2p4XERGhP/3pTzp+/Lik2uLyzTffXPT9nfH7GhcXp3/84x9av369bZvFYtHZs2cl1Z7sH3vsMc2dO9d2jKWlpUpPT9e0adOavX3vvfee+vXrpw8//FBbt27V1q1b9b//+79at26dvvvuOw0YMEAPPfSQYmJi9OWXX+qKK67Q9ddfbxs9kWp/7/+tU6dOqqystP1c3zlp//796tatm+655x499dRTdf64aSlbtmzR6tWrNWnSpDrbKysr1bNnT7Vr10579uzRrl27Wrxtza1///6qrq62HdumTZt04sSJVm2TS/SsJ02apIkTJ6pDhw764x//qLvvvltJSUmKi4uzPSYkJETp6emaM2eOqqurdfbsWd1www269tpr9eCDD+qZZ55RdHS0evTooZ///OeXfK/S0lK99NJLOnfunCwWi4YPH67Bgwe3wFE2XmBgoJKTk/XEE0+oQ4cObW6C2XlPPPGEDMPQkiVL1KdPH0VFRcnf319ms7lVTmIN8fTTT8vb21tnz57VVVddpaeeekqrVq2q85jQ0FDNnDnTdjLs06ePkpKSJEn33Xefdu/erbvuukv+/v665pprVFZWJkkaPXq0KioqbJPlDMPQ/fffr5/97Gc/aoczfl979Oih//f//p8yMjL0+uuvy8/PT15eXrrjjjs0cOBASdKUKVNkMpl077332mb1T5w4UaNHj2729mVnZ2vUqFF1tl1//fWyWq1KTEzUsWPH5OnpKV9fX82fP1+StGDBAr388suKjo6WyWRSdHS0Jk+eXOc17rjjDq1fv16xsbG2CWaXOif99a9/1YYNG9S+fXt5eHho9uzZzX7cUu339vytW1deeaWWLVum6667Th9++KHtMU8++aRmzZqlt99+W/369dONN97YIm1rSV5eXsrIyKgzwaxXr16t2iYPoyUuALWwXbt2ae7cudqwYUOrD10AAGAvl+hZX2j27Nn65JNPlJqaSqEGALgEl+xZAwDgSlxighkAAK6MYg0AgJOjWAMA4OQo1gAAODmKNQAATo5iDQCAk/v/0mF4smoQWGgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(df.isnull(), cbar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tyears</th>\n",
       "      <th>d</th>\n",
       "      <th>Karn</th>\n",
       "      <th>Broders</th>\n",
       "      <th>FIGO</th>\n",
       "      <th>Ascites</th>\n",
       "      <th>Diam</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [tyears, d, Karn, Broders, FIGO, Ascites, Diam, id]\n",
       "Index: []"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['Karn'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tyears</th>\n",
       "      <th>d</th>\n",
       "      <th>Karn</th>\n",
       "      <th>Broders</th>\n",
       "      <th>FIGO</th>\n",
       "      <th>Ascites</th>\n",
       "      <th>Diam</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [tyears, d, Karn, Broders, FIGO, Ascites, Diam, id]\n",
       "Index: []"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.duplicated(subset=['tyears','d'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (df.duplicated(subset=['tyears','d'])).sum() > 0:\n",
    "    sns.heatmap(df[df.duplicated(subset=['tyears','d'])], cbar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tyears</th>\n",
       "      <th>d</th>\n",
       "      <th>Karn</th>\n",
       "      <th>Broders</th>\n",
       "      <th>FIGO</th>\n",
       "      <th>Ascites</th>\n",
       "      <th>Diam</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [tyears, d, Karn, Broders, FIGO, Ascites, Diam, id]\n",
       "Index: []"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.duplicated(subset=['tyears'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df[~df.duplicated(subset=['tyears','d'])].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conversion to torch Tensors from_numpy() preserves precision (to Long type in torch) and then there are no ties in the data, but then torch.einsum() in the loss function will fail as it expects float type.  \n",
    "\n",
    "However when converting to float type precisition is lost and then there is a tie in two entries: the ones at positions 28 and 29 in Python, or 29 and 30 (0 indexing vs 1 indexing) after sorting entries by increasing tyears.  \n",
    "To avoid complications from ties, first drop these and try to get agreement with R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tyears</th>\n",
       "      <th>d</th>\n",
       "      <th>Karn</th>\n",
       "      <th>Broders</th>\n",
       "      <th>FIGO</th>\n",
       "      <th>Ascites</th>\n",
       "      <th>Diam</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.356164</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>IV</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&gt;5cm</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.356164</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>III</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&gt;5cm</td>\n",
       "      <td>130</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      tyears  d  Karn Broders FIGO Ascites  Diam   id\n",
       "28  0.356164  1    10       2   IV    <NA>  >5cm   25\n",
       "29  0.356164  1     8       2  III    <NA>  >5cm  130"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[[28,29],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ties_mask = df.index.isin([28,29])\n",
    "df[ties_mask]\n",
    "\n",
    "df = df[~ties_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([<NA>, 'present', 'absent'], dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.unique(df.Ascites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/pandas/core/indexing.py:1743: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(ilocs[0], value)\n"
     ]
    }
   ],
   "source": [
    "df.loc[:, 'Ascites'] = df.Ascites.replace({'present':1, 'absent':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     306\n",
       "unique      2\n",
       "top         1\n",
       "freq      212\n",
       "Name: Ascites, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Ascites.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-17-a340c44d636e>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.dropna(subset=['tyears','d','Karn','Ascites'], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "df.dropna(subset=['tyears','d','Karn','Ascites'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/pandas/core/indexing.py:1743: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(ilocs[0], value)\n"
     ]
    }
   ],
   "source": [
    "df.loc[:, 'Ascites'] = df.Ascites.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ties_mask2 = df.index.isin([304,305])\n",
    "df[ties_mask2]\n",
    "\n",
    "df = df[~ties_mask2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv('../data/ovarian_deduplicated.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "tname = 'tyears'\n",
    "Xnames = ['Karn', 'Ascites']\n",
    "dname = 'd'\n",
    "\n",
    "beta = nn.Parameter(torch.zeros(df[Xnames].shape[1])).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.from_numpy(df[tname].values).float()\n",
    "X = torch.from_numpy(df[Xnames].values).float()\n",
    "d = torch.from_numpy(df[dname].values).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "304"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.unique(t).shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "304"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.unique(t).shape[0] == t.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniqt, uniqt_cnt = torch.unique(t, return_counts=True)\n",
    "uniqt[uniqt_cnt > 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For explanation of Cox likelihood see page 127 (page 142 in PDF) of this text:  \n",
    "http://references.tomhsiung.com/Medical%20Statistics/Survival%20Analysis%20-%20A%20Self-learning%20Text%203E.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of how to code up loss optimisation in PyTorch, towards the end of these slides:  \n",
    "https://github.com/mlelarge/dataflowr-slides/blob/master/Slides/02_neural_nets_dldiy/pres.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For discussion on different ways to handle ties in event time, see page 15 here:  \n",
    "http://www-personal.umich.edu/~yili/lect4notes.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(t, X, d, beta):\n",
    "    \n",
    "    tevents = torch.unique(t[d == 1])\n",
    "    \n",
    "    loss = 0\n",
    "    for time in tevents:\n",
    "                \n",
    "        Xcurrevent = X[t == time]\n",
    "        Xatrisk = X[t >= time]\n",
    "                \n",
    "        multiplicity = 1\n",
    "        \n",
    "        #If there are ties, apply Breslow correction, see slide 15 here:\n",
    "        #http://www-personal.umich.edu/~yili/lect4notes.pdf\n",
    "        if Xcurrevent.shape[0]>1:\n",
    "            multiplicity = Xcurrevent.shape[0]\n",
    "            #print(\"TIE observed, multiplicity: {}\".format(multiplicity))\n",
    "            Xcurrevent = torch.reshape(Xcurrevent[0,:], (1, len(beta)))\n",
    "        \n",
    "        loss += torch.einsum('j,ij->i', beta, Xcurrevent) #torch.dot(beta, Xcurrevent)\n",
    "        loss += -multiplicity*torch.log(torch.sum(torch.exp(torch.einsum('j,ij->i', beta, Xatrisk))))\n",
    "    \n",
    "    return -loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1160.4219], grad_fn=<NegBackward>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_loss(t, X, d, beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.14 s, sys: 13.5 ms, total: 7.16 s\n",
      "Wall time: 7.15 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.23625141,  0.41612804], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "optimizer = optim.LBFGS([beta], lr=0.1, max_iter=1e3)\n",
    "#optimizer = optim.SGD([beta], lr=0.001)\n",
    "#optimizer = optim.RMSprop([beta], lr=0.001)\n",
    "\n",
    "\n",
    "def closure():\n",
    "    optimizer.zero_grad()\n",
    "    loss = get_loss(t, X, d, beta)\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "optimizer.step(closure)\n",
    "\n",
    "betas = beta.detach().numpy()\n",
    "betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sudo pip3 install rpy2\n",
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: \n",
      "Attaching package: â€˜dplyrâ€™\n",
      "\n",
      "\n",
      "R[write to console]: The following objects are masked from â€˜package:statsâ€™:\n",
      "\n",
      "    filter, lag\n",
      "\n",
      "\n",
      "R[write to console]: The following objects are masked from â€˜package:baseâ€™:\n",
      "\n",
      "    intersect, setdiff, setequal, union\n",
      "\n",
      "\n",
      "R[write to console]: Parsed with column specification:\n",
      "cols(\n",
      "  tyears = \u001b[32mcol_double()\u001b[39m,\n",
      "  d = \u001b[32mcol_double()\u001b[39m,\n",
      "  Karn = \u001b[32mcol_double()\u001b[39m,\n",
      "  Broders = \u001b[31mcol_character()\u001b[39m,\n",
      "  FIGO = \u001b[31mcol_character()\u001b[39m,\n",
      "  Ascites = \u001b[31mcol_character()\u001b[39m,\n",
      "  Diam = \u001b[31mcol_character()\u001b[39m,\n",
      "  id = \u001b[32mcol_double()\u001b[39m\n",
      ")\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[90m# A tibble: 4 x 8\u001b[39m\n",
      "  tyears     d  Karn Broders FIGO  Ascites Diam     id\n",
      "   \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m   \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m   \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m\n",
      "\u001b[90m1\u001b[39m  0.356     1    10 2       IV    unknown >5cm     25\n",
      "\u001b[90m2\u001b[39m  0.356     1     8 2       III   unknown >5cm    130\n",
      "\u001b[90m3\u001b[39m  5.26      1     9 2       III   absent  <1cm    165\n",
      "\u001b[90m4\u001b[39m  5.26      0     9 1       III   absent  >5cm    221\n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "library(readr)\n",
    "library(survival)\n",
    "library(dplyr)\n",
    "library(tidyr)\n",
    "\n",
    "df2 = read_csv(\"../data/ovarian.csv\")\n",
    "df2 %>% arrange(tyears) %>% slice(c(29,30,305,306))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Karn    Ascites \n",
      "-0.2364006  0.4170308 \n",
      "Time difference of 0.03108072 secs\n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "\n",
    "df2 = df2 %>% arrange(tyears) %>% slice(c(-29,-30,-305,-306))\n",
    "\n",
    "df2 = df2 %>% mutate(Ascites = case_when(Ascites=='unknown'~NA_real_,\n",
    "                                  Ascites=='present'~1,\n",
    "                                  Ascites=='absent'~0))\n",
    "\n",
    "df2 = df2 %>% drop_na(tyears,d,Karn,Ascites)\n",
    "\n",
    "\n",
    "starttime = Sys.time()\n",
    "\n",
    "rmod = coxph(Surv(tyears, d) ~ Karn + Ascites, df2, ties=\"breslow\")\n",
    "print(coef(rmod))\n",
    "\n",
    "endtime = Sys.time()\n",
    "\n",
    "print(endtime-starttime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute second derivative of likelihood to get confidence intervals?  \n",
    "https://discuss.pytorch.org/t/how-to-calculate-2nd-derivative-of-a-likelihood-function/15085/8  \n",
    "\n",
    "https://personal.psu.edu/abs12/stat504/Lecture/lec3_4up.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from torch.autograd import grad\n",
    "\n",
    "loss = get_loss(t, X, d)\n",
    "\n",
    "loss_grads = grad(loss, beta, create_graph=True)\n",
    "\n",
    "# compute the second order derivative w.r.t. each parameter\n",
    "d2loss = []\n",
    "for idx, param, grd in zip(range(beta.shape[0]), beta, loss_grads):\n",
    "    #print(idx, param, grd)\n",
    "    drv = grad(grd, param, create_graph=True)\n",
    "    d2loss.append(drv)\n",
    "    print(param, drv)\n",
    "\n",
    "#for param, grd in zip(beta, loss_grads):\n",
    "#    print(param)\n",
    "#    for idx,_ in enumerate(beta):\n",
    "#        print(grd)\n",
    "#        drv = grad(grd[idx], param[idx], create_graph=True)\n",
    "#        d2loss.append(drv)\n",
    "#    print(param, drv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baselineHazard(df, betas, Xnames, tname):\n",
    "    \"\"\"Compute the baseline hazard using the Breslow estimator for it.\"\"\"\n",
    "    \n",
    "    t = np.sort(df[tname].values)\n",
    "    t_uniq = np.unique(t)\n",
    "    \n",
    "    h0 = []\n",
    "    for time in t_uniq:\n",
    "        \n",
    "        value = 1/np.sum(np.exp(np.dot(df.loc[df[tname] >= time, Xnames].values, betas)))\n",
    "        h0.append({'time':time, 'h0':value})\n",
    "\n",
    "    return pd.DataFrame(h0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>h0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.024657</td>\n",
       "      <td>0.018757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.027398</td>\n",
       "      <td>0.018838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.035617</td>\n",
       "      <td>0.018969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.041094</td>\n",
       "      <td>0.019052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.082192</td>\n",
       "      <td>0.019122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>7.060274</td>\n",
       "      <td>1.443338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>7.112330</td>\n",
       "      <td>1.670411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>7.120548</td>\n",
       "      <td>2.193627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>7.290410</td>\n",
       "      <td>3.636039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>7.476712</td>\n",
       "      <td>10.617612</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>304 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         time         h0\n",
       "0    0.024657   0.018757\n",
       "1    0.027398   0.018838\n",
       "2    0.035617   0.018969\n",
       "3    0.041094   0.019052\n",
       "4    0.082192   0.019122\n",
       "..        ...        ...\n",
       "299  7.060274   1.443338\n",
       "300  7.112330   1.670411\n",
       "301  7.120548   2.193627\n",
       "302  7.290410   3.636039\n",
       "303  7.476712  10.617612\n",
       "\n",
       "[304 rows x 2 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baselineHazard(df, betas, Xnames, tname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          time        haz\n",
      "1   0.02465724 0.01884820\n",
      "2   0.02739762 0.01897962\n",
      "3   0.03561696 0.01906246\n",
      "4   0.04109387 0.01913217\n",
      "5   0.08219248 0.01923892\n",
      "6   0.08767131 0.01937587\n",
      "7   0.11232937 0.01944789\n",
      "8   0.12328726 0.01958784\n",
      "9   0.12876654 0.01972982\n",
      "10  0.13150880 0.01980043\n",
      "11  0.18082221 0.01987156\n",
      "12  0.18904309 0.01994319\n",
      "13  0.20821886 0.02005922\n",
      "14  0.22739782 0.02015177\n",
      "15  0.24657466 0.02024519\n",
      "16  0.26575480 0.02030722\n",
      "17  0.28493007 0.02045986\n",
      "18  0.30684943 0.02049930\n",
      "19  0.30684997 0.02055945\n",
      "20  0.36438403 0.02063614\n",
      "21  0.36712414 0.02076039\n",
      "22  0.37260313 0.02088615\n",
      "23  0.37808433 0.02096530\n",
      "24  0.38630286 0.02112803\n",
      "25  0.40000226 0.02119193\n",
      "26  0.41917974 0.02129526\n",
      "27  0.44109518 0.02142760\n",
      "28  0.45753364 0.02149332\n",
      "29  0.47945292 0.02157715\n",
      "30  0.50137037 0.02166164\n",
      "31  0.50137066 0.02179859\n",
      "32  0.50959059 0.02184337\n",
      "33  0.51233045 0.02192996\n",
      "34  0.51506886 0.02204063\n",
      "35  0.52328670 0.02215243\n",
      "36  0.52328814 0.02229567\n",
      "37  0.52602734 0.02236684\n",
      "38  0.53698666 0.02245763\n",
      "39  0.54246531 0.02264446\n",
      "40  0.55068681 0.02279416\n",
      "41  0.56438502 0.02284313\n",
      "42  0.57260391 0.02293784\n",
      "43  0.60547866 0.02301317\n",
      "44  0.61643724 0.02310930\n",
      "45  0.61643878 0.02318577\n",
      "46  0.62191925 0.02328335\n",
      "47  0.64657641 0.02338176\n",
      "48  0.64931382 0.02346004\n",
      "49  0.65205567 0.02352578\n",
      "50  0.66575332 0.02360503\n",
      "51  0.68219222 0.02376775\n",
      "52  0.68219291 0.02387030\n",
      "53  0.70410700 0.02392401\n",
      "54  0.72602610 0.02402792\n",
      "55  0.73424635 0.02419654\n",
      "56  0.73424802 0.02428038\n",
      "57  0.74246432 0.02438741\n",
      "58  0.75068561 0.02447749\n",
      "59  0.75342352 0.02453396\n",
      "60  0.77808274 0.02464325\n",
      "61  0.80822089 0.02482065\n",
      "62  0.84383554 0.02489425\n",
      "63  0.84931274 0.02495267\n",
      "64  0.86301307 0.02513457\n",
      "65  0.90958928 0.02531914\n",
      "66  0.94246708 0.02537957\n",
      "67  0.94520489 0.02544029\n",
      "68  0.96438372 0.02553299\n",
      "69  0.96712374 0.02565138\n",
      "70  0.97534167 0.02580293\n",
      "71  1.01095930 0.02595628\n",
      "72  1.02191881 0.02607864\n",
      "73  1.02739863 0.02617605\n",
      "74  1.02739924 0.02630050\n",
      "75  1.05479398 0.02639958\n",
      "76  1.05753324 0.02660327\n",
      "77  1.08767011 0.02673182\n",
      "78  1.09315109 0.02689645\n",
      "79  1.10410967 0.02696465\n",
      "80  1.12876687 0.02706881\n",
      "81  1.14246657 0.02717378\n",
      "82  1.14794584 0.02727957\n",
      "83  1.15342518 0.02738618\n",
      "84  1.16712407 0.02749363\n",
      "85  1.16986219 0.02756490\n",
      "86  1.18356042 0.02767376\n",
      "87  1.18904188 0.02795800\n",
      "88  1.19452244 0.02813812\n",
      "89  1.21369843 0.02843203\n",
      "90  1.22191715 0.02861834\n",
      "91  1.23561561 0.02880710\n",
      "92  1.24657607 0.02904981\n",
      "93  1.25479422 0.02917074\n",
      "94  1.25753587 0.02948673\n",
      "95  1.26301376 0.02956873\n",
      "96  1.27123169 0.02967325\n",
      "97  1.27123414 0.02984250\n",
      "98  1.27397280 0.02997013\n",
      "99  1.27671389 0.03007751\n",
      "100 1.33150871 0.03028608\n",
      "101 1.35342510 0.03041755\n",
      "102 1.35616352 0.03055016\n",
      "103 1.36438474 0.03063818\n",
      "104 1.36438485 0.03075041\n",
      "105 1.36712091 0.03092229\n",
      "106 1.39452054 0.03103662\n",
      "107 1.39726135 0.03121172\n",
      "108 1.41917711 0.03138881\n",
      "109 1.46027372 0.03156793\n",
      "110 1.46301272 0.03171078\n",
      "111 1.47397437 0.03200513\n",
      "112 1.47671095 0.03220212\n",
      "113 1.47945194 0.03240154\n",
      "114 1.49041064 0.03255206\n",
      "115 1.50136859 0.03275585\n",
      "116 1.52876816 0.03295096\n",
      "117 1.55342449 0.03310663\n",
      "118 1.56712223 0.03330595\n",
      "119 1.59178288 0.03346501\n",
      "120 1.61095975 0.03359895\n",
      "121 1.61643784 0.03370545\n",
      "122 1.62739512 0.03403819\n",
      "123 1.63561781 0.03420433\n",
      "124 1.69314970 0.03463961\n",
      "125 1.69588949 0.03481169\n",
      "126 1.70137072 0.03526266\n",
      "127 1.72328677 0.03541141\n",
      "128 1.73972685 0.03563954\n",
      "129 1.75616392 0.03588397\n",
      "130 1.77808304 0.03600548\n",
      "131 1.81643803 0.03630478\n",
      "132 1.83013638 0.03654460\n",
      "133 1.85753370 0.03667063\n",
      "134 1.87123361 0.03691533\n",
      "135 1.88218972 0.03723001\n",
      "136 1.89315225 0.03736082\n",
      "137 1.90685106 0.03752784\n",
      "138 1.91232914 0.03785310\n",
      "139 1.94246444 0.03805869\n",
      "140 1.95890279 0.03848347\n",
      "141 1.95890431 0.03875305\n",
      "142 1.98082162 0.03902643\n",
      "143 1.98356120 0.03925761\n",
      "144 1.98904050 0.03953818\n",
      "145 1.98904067 0.03989939\n",
      "146 1.99999927 0.04004967\n",
      "147 2.00821842 0.04020108\n",
      "148 2.01369783 0.04080370\n",
      "149 2.01643777 0.04104269\n",
      "150 2.02739694 0.04128449\n",
      "151 2.10684877 0.04159490\n",
      "152 2.12054783 0.04191002\n",
      "153 2.13424599 0.04242571\n",
      "154 2.14520258 0.04275358\n",
      "155 2.15616310 0.04301603\n",
      "156 2.16164402 0.04344392\n",
      "157 2.17534330 0.04399830\n",
      "158 2.17808262 0.04418111\n",
      "159 2.18082183 0.04446143\n",
      "160 2.18356035 0.04482166\n",
      "161 2.26027473 0.04528642\n",
      "162 2.27123421 0.04576092\n",
      "163 2.27397113 0.04614260\n",
      "164 2.30137153 0.04644846\n",
      "165 2.31506909 0.04665224\n",
      "166 2.32054736 0.04685782\n",
      "167 2.38629944 0.04750340\n",
      "168 2.52054852 0.04782762\n",
      "169 2.52328935 0.04815630\n",
      "170 2.52602886 0.04848953\n",
      "171 2.56712272 0.04877124\n",
      "172 2.57260117 0.04911306\n",
      "173 2.58629930 0.04945971\n",
      "174 2.61917785 0.04981128\n",
      "175 2.63013806 0.05026386\n",
      "176 2.63835593 0.05062700\n",
      "177 2.66575332 0.05099543\n",
      "178 2.68492981 0.05136926\n",
      "179 2.72054835 0.05185072\n",
      "180 2.74794625 0.05285759\n",
      "181 2.76438334 0.05325933\n",
      "182 2.81917917 0.05359938\n",
      "183 2.83835614 0.05394380\n",
      "184 2.88219159 0.05421886\n",
      "185 2.91780672 0.05464164\n",
      "186 2.98082243 0.05499963\n",
      "187 3.01095995 0.05570108\n",
      "188 3.03287650 0.05642066\n",
      "189 3.03835642 0.05720200\n",
      "190 3.09863017 0.05779965\n",
      "191 3.11781061 0.05857484\n",
      "192 3.18082213 0.05906859\n",
      "193 3.25479496 0.05970610\n",
      "194 3.28219095 0.06004323\n",
      "195 3.29041114 0.06038420\n",
      "196 3.29863045 0.06090906\n",
      "197 3.30136984 0.06135423\n",
      "198 3.32876815 0.06204231\n",
      "199 3.33424665 0.06259652\n",
      "200 3.42191676 0.06375396\n",
      "201 3.69588973 0.06413851\n",
      "202 3.72602962 0.06452773\n",
      "203 3.73972663 0.06512745\n",
      "204 3.79726039 0.06573842\n",
      "205 3.81369850 0.06636097\n",
      "206 3.89315013 0.06677771\n",
      "207 3.94246576 0.06781459\n",
      "208 4.00000016 0.06847728\n",
      "209 4.07397293 0.06915305\n",
      "210 4.10958748 0.00000000\n",
      "211 4.16438414 0.00000000\n",
      "212 4.18356082 0.00000000\n",
      "213 4.18904107 0.07284657\n",
      "214 4.21369831 0.00000000\n",
      "215 4.22191907 0.00000000\n",
      "216 4.22465640 0.00000000\n",
      "217 4.23561597 0.00000000\n",
      "218 4.24657571 0.07677801\n",
      "219 4.27397245 0.00000000\n",
      "220 4.28219157 0.00000000\n",
      "221 4.28493218 0.00000000\n",
      "222 4.32328669 0.00000000\n",
      "223 4.32328848 0.00000000\n",
      "224 4.33972634 0.00000000\n",
      "225 4.35616347 0.08407812\n",
      "226 4.36986315 0.00000000\n",
      "227 4.41095739 0.00000000\n",
      "228 4.43013895 0.00000000\n",
      "229 4.43835840 0.00000000\n",
      "230 4.46575408 0.00000000\n",
      "231 4.47397191 0.00000000\n",
      "232 4.51232871 0.00000000\n",
      "233 4.56164379 0.00000000\n",
      "234 4.58630247 0.00000000\n",
      "235 4.64657556 0.00000000\n",
      "236 4.64931278 0.00000000\n",
      "237 4.70684981 0.00000000\n",
      "238 4.72328791 0.09945913\n",
      "239 4.73698619 0.00000000\n",
      "240 4.76164393 0.00000000\n",
      "241 4.77534324 0.10533232\n",
      "242 4.77808201 0.00000000\n",
      "243 4.86849333 0.00000000\n",
      "244 4.95616332 0.00000000\n",
      "245 4.96986386 0.00000000\n",
      "246 4.97534327 0.00000000\n",
      "247 4.99178156 0.11735998\n",
      "248 5.00821909 0.00000000\n",
      "249 5.05205574 0.00000000\n",
      "250 5.07123330 0.12639082\n",
      "251 5.08766945 0.00000000\n",
      "252 5.09041060 0.13137008\n",
      "253 5.10410752 0.00000000\n",
      "254 5.12876556 0.00000000\n",
      "255 5.14246576 0.13902256\n",
      "256 5.14246594 0.00000000\n",
      "257 5.14520525 0.00000000\n",
      "258 5.15890496 0.14522495\n",
      "259 5.16164409 0.00000000\n",
      "260 5.19726090 0.00000000\n",
      "261 5.25205526 0.00000000\n",
      "262 5.26027437 0.16380482\n",
      "263 5.29589206 0.00000000\n",
      "264 5.31506715 0.17296969\n",
      "265 5.32876637 0.00000000\n",
      "266 5.43287667 0.00000000\n",
      "267 5.46575146 0.00000000\n",
      "268 5.46849222 0.00000000\n",
      "269 5.51232760 0.00000000\n",
      "270 5.51233203 0.00000000\n",
      "271 5.52876687 0.00000000\n",
      "272 5.58356057 0.00000000\n",
      "273 5.66301402 0.21138768\n",
      "274 5.66849293 0.21796276\n",
      "275 5.73150723 0.00000000\n",
      "276 5.74246475 0.00000000\n",
      "277 5.75068364 0.25469676\n",
      "278 5.86849255 0.00000000\n",
      "279 5.89041174 0.00000000\n",
      "280 5.89589004 0.00000000\n",
      "281 5.93424720 0.00000000\n",
      "282 6.10684884 0.00000000\n",
      "283 6.14520547 0.00000000\n",
      "284 6.18356073 0.00000000\n",
      "285 6.20000156 0.00000000\n",
      "286 6.21095848 0.00000000\n",
      "287 6.33424791 0.00000000\n",
      "288 6.33698698 0.00000000\n",
      "289 6.40274076 0.00000000\n",
      "290 6.50684951 0.00000000\n",
      "291 6.54794518 0.00000000\n",
      "292 6.54794592 0.00000000\n",
      "293 6.61643718 0.00000000\n",
      "294 6.65753416 0.00000000\n",
      "295 6.70137292 0.00000000\n",
      "296 6.81643894 0.00000000\n",
      "297 6.85479430 0.00000000\n",
      "298 6.90684886 0.00000000\n",
      "299 7.04109416 0.00000000\n",
      "300 7.06027420 0.00000000\n",
      "301 7.11232982 0.00000000\n",
      "302 7.12054780 0.00000000\n",
      "303 7.29040977 0.00000000\n",
      "304 7.47671221         NA\n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "\n",
    "basehaz(rmod, centered=FALSE) %>% rename(cumhaz=hazard) %>% mutate(haz=lead(cumhaz)-cumhaz) %>% select(time, haz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>time</th>\n",
       "      <th>status</th>\n",
       "      <th>smoke</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bob</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sally</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>James</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ann</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  time  status  smoke\n",
       "0    Bob     1       1      1\n",
       "1  Sally     3       1      0\n",
       "2  James     6       0      0\n",
       "3    Ann    10       1      1"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valdf = pd.DataFrame({'id':['Bob','Sally','James','Ann'], 'time':[1,3,6,10], 'status':[1,1,0,1], 'smoke':[1,0,0,1]})\n",
    "valdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify model Cox(t,d ~ smoke) for times $t$ and status indicator $d$.  \n",
    "$L(T, D, X) = \\prod_{t_i^e} \\frac{h(t_i^e,X_i)}{\\sum_{j:\\, t_j \\geq t_i^e} h(t_j, X_j)}$  \n",
    "\n",
    "where $t_i^e$ denote event times (i.e. times where $d=1$), and $t_i$ denotes all times (whether event or censored).  \n",
    "\n",
    "In this way the ratio $\\frac{h(t_i^e,X_i)}{\\sum_{j:\\, t_j \\geq t_i} h(t_j, X_j)}$ is the ratio of the hazard for the event observed at time $t_i^e$ divided by the sum of hazards for all subjects at risk at that time (so with $t_j \\geq t_i^e$, i.e. have not had an event yet, have not died, so still at risk, including the subject who had an event at that time since that person must have been at risk too, so inequality includes time $t_i^e$).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We therefore see that for each of the subjects in the data just above the hazard, $h(t, X) = h_0(t)\\,\\exp(X \\beta)$, is:  \n",
    "- Bob: $h(t, X) = h_0(t)\\,\\exp(\\beta)$\n",
    "- Sally: $h(t, X) = h_0(t)\\,\\exp(0)$\n",
    "- James: $h(t, X) = h_0(t)\\,\\exp(0)$\n",
    "- Ann: $h(t, X) = h_0(t)\\,\\exp(\\beta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The likelihood for this dataset then has three factors, one for each event time, and in each factor is the hazard of the person who had the event divided by the sum of the hazards of all the people who were still at risk then (including the person who had the event:  \n",
    "  \n",
    "$\n",
    "\\begin{align}\n",
    "L(T, D, X) = &\\left[\\frac{h_0(t)\\exp(\\beta)}{h_0(t)\\exp(\\beta) \\,+\\, h_0(t)\\exp(0) \\,+\\, h_0(t)\\exp(0) \\,+\\, h_0(t)\\exp(\\beta)} \\right] \\\\\n",
    "&\\times \\left[ \\frac{h_0(t)\\exp(0)}{h_0(t)\\exp(0) \\,+\\, h_0(t)\\exp(0) \\,+\\, h_0(t)\\exp(\\beta)} \\right] \\\\\n",
    "&\\times \\left[ \\frac{h_0(t)\\exp(\\beta)}{h_0(t)\\exp(\\beta)} \\right]\n",
    "\\end{align}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the baseline hazard, $h_0(t)$ will always cancel everywhere in the likelihood! We then have:  \n",
    "  \n",
    "$\\begin{align}\n",
    "L(T, D, X) &= \\left[\\frac{\\exp(\\beta)}{2(1 + \\exp(\\beta))} \\right] \\times \\left[ \\frac{1}{2+ \\exp(\\beta)} \\right] \\times 1\n",
    "\\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the natural logarithm of this as the logarithm is a monotone transformation and will not change the position of the minimum, but will simplify the computation of the derivative and make the computation more numerically stable. Also we multipy it by $-1$ so that to maximise the likelihood we minimise the quantity below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$-\\ln[L(T,D,X)] = -\\beta +\\ln[2(1+\\exp(\\beta)] - \\ln[1] + \\ln[2+\\exp(\\beta)]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking the derivative of this wrt $\\beta$ gives  \n",
    "$\\begin{align}\n",
    "-\\frac{d}{d\\beta} \\ln[L(T,D,X)] &= -1 + \\frac{d}{d\\beta}\\ln[(1+\\exp(\\beta)] + \\frac{d}{d\\beta}\\ln[2+\\exp(\\beta)] \\\\\n",
    "&= -1 + \\frac{1}{1+\\exp(\\beta)}\\exp(\\beta) + \\frac{1}{2+\\exp(\\beta)}\\exp(\\beta)\n",
    "\\end{align}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting this equal to zero, $-\\frac{d}{d\\beta} \\ln[L(T,D,X)] = 0$, gives  \n",
    "$1 = \\frac{\\exp(\\beta)}{1+\\exp(\\beta)} + \\frac{\\exp(\\beta)}{2+\\exp(\\beta)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that this is the equation we need to solve for $\\beta$ to find the stationary point (hopefully a minimum, but we haven't proven this, or even convexity yet), and this will be the Maximum Likelihood Estimate (MLE) for $\\beta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives $\\beta = \\ln(2)/2 \\approx 0.34657$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.34426704], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tname_val = 'time'\n",
    "Xnames_val = ['smoke']\n",
    "dname_val = 'status'\n",
    "\n",
    "beta_val = nn.Parameter(torch.zeros(valdf[Xnames_val].shape[1])).float()\n",
    "\n",
    "tval = torch.from_numpy(valdf[tname_val].values).float()\n",
    "Xval = torch.from_numpy(valdf[Xnames_val].values).float()\n",
    "dval = torch.from_numpy(valdf[dname_val].values).float()\n",
    "\n",
    "optimizer_val = optim.LBFGS([beta_val], lr=0.1, max_iter=1e3)\n",
    "\n",
    "def closure():\n",
    "    optimizer_val.zero_grad()\n",
    "    loss_val = get_loss(tval, Xval, dval, beta_val)\n",
    "    loss_val.backward()\n",
    "    return loss_val\n",
    "\n",
    "optimizer_val.step(closure)\n",
    "\n",
    "betas_val = beta_val.detach().numpy()\n",
    "betas_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "\n",
    "class Torch_Cox(BaseEstimator):\n",
    "    \"\"\"Fit a Cox model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.1, random_state=None):\n",
    "        self.random_state = random_state\n",
    "        self.lr = lr\n",
    "        \n",
    "    def get_loss(t, X, d, beta):\n",
    "        tevents = torch.unique(t[d == 1])\n",
    "\n",
    "        loss = 0\n",
    "        for time in tevents:\n",
    "\n",
    "            Xcurrevent = X[t == time]\n",
    "            Xatrisk = X[t >= time]\n",
    "\n",
    "            multiplicity = 1\n",
    "\n",
    "            #If there are ties, apply Breslow correction, see slide 15 here:\n",
    "            #http://www-personal.umich.edu/~yili/lect4notes.pdf\n",
    "            if Xcurrevent.shape[0]>1:\n",
    "                multiplicity = Xcurrevent.shape[0]\n",
    "                #print(\"TIE observed, multiplicity: {}\".format(multiplicity))\n",
    "                Xcurrevent = torch.reshape(Xcurrevent[0,:], (1, len(beta)))\n",
    "\n",
    "            loss += torch.einsum('j,ij->i', beta, Xcurrevent) #torch.dot(beta, Xcurrevent)\n",
    "            loss += -multiplicity*torch.log(torch.sum(torch.exp(torch.einsum('j,ij->i', beta, Xatrisk))))\n",
    "\n",
    "        return -loss\n",
    "\n",
    "    # the arguments are ignored anyway, so we make them optional\n",
    "    def fit(self, df, Xnames=None, tname=None, dname=None):\n",
    "        #self.random_state_ = check_random_state(self.random_state)\n",
    "        beta = nn.Parameter(torch.zeros(df[Xnames].shape[1])).float()\n",
    "        \n",
    "        t = torch.from_numpy(df[tname].values).float()\n",
    "        X = torch.from_numpy(df[Xnames].values).float()\n",
    "        d = torch.from_numpy(df[dname].values).float()\n",
    "        \n",
    "        optimizer = optim.LBFGS([beta], lr=self.lr, max_iter=1e3)\n",
    "\n",
    "        def closure():\n",
    "            optimizer.zero_grad()\n",
    "            loss = get_loss(t, X, d, beta)\n",
    "            loss.backward()\n",
    "            return loss\n",
    "\n",
    "        optimizer.step(closure)\n",
    "\n",
    "        betas = beta.detach().numpy()\n",
    "        return betas\n",
    "    #def predict(self, Xtest):\n",
    "    #    n_samples = X.shape[0]\n",
    "    #    return self.random_state_.randn(n_samples, n_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.64 s, sys: 51.6 ms, total: 7.69 s\n",
      "Wall time: 7.73 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.23625141,  0.41612804], dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "coxmod = Torch_Cox()\n",
    "\n",
    "coxmod.fit(df, Xnames=Xnames, tname=tname, dname=dname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padToMatch2d(inputtens, targetshape):\n",
    "    target = torch.zeros(*targetshape)\n",
    "    target[:inputtens.shape[0], :inputtens.shape[1]] = inputtens\n",
    "    return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.9422, -0.5937,  1.4452, -0.4053],\n",
       "        [ 0.0663,  0.6285,  0.7254,  0.6593],\n",
       "        [-0.3980, -0.6343,  0.4762, -0.3063],\n",
       "        [ 0.7697, -0.0324, -0.8594, -0.0328],\n",
       "        [ 1.4040, -0.9355,  0.4580,  0.4798],\n",
       "        [-0.0400, -1.9020,  1.6582,  0.4812],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testtens1 = torch.randn(6,4)\n",
    "testtens2 = torch.randn(10,4)\n",
    "\n",
    "padToMatch2d(testtens1, testtens2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0247,  1.0000,  8.0000,  1.0000],\n",
       "         [ 0.0274,  1.0000,  6.0000,  1.0000],\n",
       "         [ 0.0356,  1.0000,  8.0000,  1.0000],\n",
       "         ...,\n",
       "         [ 7.1205,  0.0000,  9.0000,  1.0000],\n",
       "         [ 7.2904,  0.0000,  9.0000,  1.0000],\n",
       "         [ 7.4767,  0.0000, 10.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.0274,  1.0000,  6.0000,  1.0000],\n",
       "         [ 0.0356,  1.0000,  8.0000,  1.0000],\n",
       "         [ 0.0411,  1.0000,  7.0000,  0.0000],\n",
       "         ...,\n",
       "         [ 7.2904,  0.0000,  9.0000,  1.0000],\n",
       "         [ 7.4767,  0.0000, 10.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.0356,  1.0000,  8.0000,  1.0000],\n",
       "         [ 0.0411,  1.0000,  7.0000,  0.0000],\n",
       "         [ 0.0822,  1.0000,  7.0000,  1.0000],\n",
       "         ...,\n",
       "         [ 7.4767,  0.0000, 10.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000]]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tevent = torch.unique(t)\n",
    "\n",
    "tensin = torch.from_numpy(df[[tname,dname,*Xnames]].sort_values(tname).values)\n",
    "\n",
    "tensin_events = tensin[tensin[:,1]==1,]\n",
    "tensin_events = torch.unique(tensin_events[:,0])\n",
    "\n",
    "tensor = torch.stack([padToMatch2d(tensin[tensin[:,0] >= eventtime, :], tensin.shape) for eventtime in tensin_events])\n",
    "\n",
    "tensor[:3,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([225, 304, 4])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[(tensor[slice, :, 0]==tensor[slice,0,0]) & (tensor[slice, :, 1]==1) for slice in range(tensor.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True, False, False,  ..., False, False, False],\n",
       "        [ True, False, False,  ..., False, False, False],\n",
       "        [ True, False, False,  ..., False, False, False],\n",
       "        ...,\n",
       "        [ True, False, False,  ..., False, False, False],\n",
       "        [ True, False, False,  ..., False, False, False],\n",
       "        [ True, False, False,  ..., False, False, False]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "event_tens_idx = torch.stack([(tensor[slice, :, 0]==tensor[slice,0,0]) & (tensor[slice, :, 1]==1) for slice in range(tensor.shape[0])])\n",
    "event_tens_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0247, 1.0000, 8.0000, 1.0000],\n",
       "        [0.0274, 1.0000, 6.0000, 1.0000],\n",
       "        [0.0356, 1.0000, 8.0000, 1.0000],\n",
       "        [0.0411, 1.0000, 7.0000, 0.0000],\n",
       "        [0.0822, 1.0000, 7.0000, 1.0000],\n",
       "        [0.0877, 1.0000, 6.0000, 1.0000],\n",
       "        [0.1123, 1.0000, 7.0000, 0.0000],\n",
       "        [0.1233, 1.0000, 6.0000, 1.0000],\n",
       "        [0.1288, 1.0000, 6.0000, 1.0000],\n",
       "        [0.1315, 1.0000, 9.0000, 1.0000]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "event_tens = tensor[event_tens_idx]\n",
    "event_tens[:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([225, 4])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "event_tens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.48 s, sys: 6.97 ms, total: 1.49 s\n",
      "Wall time: 399 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.23535603,  0.41244242], dtype=float32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "beta_val2 = nn.Parameter(torch.zeros(len(Xnames))).float()\n",
    "\n",
    "optimizer_val2 = optim.LBFGS([beta_val2], lr=0.1, max_iter=1e3)\n",
    "\n",
    "def padToMatch2d(inputtens, targetshape):\n",
    "    target = torch.zeros(*targetshape)\n",
    "    target[:inputtens.shape[0], :inputtens.shape[1]] = inputtens\n",
    "    return target\n",
    "\n",
    "tensin = torch.from_numpy(df[[tname,dname,*Xnames]].sort_values(tname).values)\n",
    "\n",
    "#Get unique event times\n",
    "tensin_events = tensin[tensin[:,1]==1,]\n",
    "tensin_events = torch.unique(tensin_events[:,0])\n",
    "\n",
    "#For each unique event stack another matrix with event at the top, and all at risk entries below\n",
    "tensor = torch.stack([padToMatch2d(tensin[tensin[:,0] >= eventtime, :], tensin.shape) for eventtime in tensin_events])\n",
    "\n",
    "#Split off the entries corresponding to events\n",
    "event_tens = tensor[:,0,:]\n",
    "#event_tens_idx = torch.stack([(tensor[slice, :, 0]==tensor[slice,0,0]) & (tensor[slice, :, 1]==1) for slice in range(tensor.shape[0])])\n",
    "#event_tens = tensor[event_tens_idx]\n",
    "\n",
    "#Drop time and status columns as no longer required\n",
    "tensor = tensor[:,:,2:]\n",
    "#nonzero_mask = torch.abs(tensor).sum(dim=2) != 0\n",
    "#tensor = tensor[nonzero_mask]\n",
    "\n",
    "event_tens = event_tens[:,2:]\n",
    "\n",
    "#print(tensor)\n",
    "\n",
    "\n",
    "def get_loss2(tensor, event_tens, beta):\n",
    "    #tevents = torch.unique(t[d == 1])\n",
    "    \n",
    "    #i indexes unique event time slices\n",
    "    #j indexes rows\n",
    "    #k indexes columns\n",
    "\n",
    "    #Numerator entries in log loss, for events\n",
    "    loss_event = torch.einsum('ik,k->i', event_tens, beta)\n",
    "    #if len(event_tens.shape)==2:\n",
    "    #    loss_event = torch.einsum('ik,k->i', event_tens, beta)\n",
    "    #elif len(event_tens.shape)==3:\n",
    "    #    loss_event = torch.einsum('ijk,k->i', event_tens, beta)\n",
    "    \n",
    "    #Denominator entries in log loss, for at-risk entries\n",
    "    #print(loss_event)\n",
    "    #print(torch.einsum('ij, j->i', tensor, beta))\n",
    "    #print(torch.einsum('ijk,k->ij', tensor, beta))\n",
    "    \n",
    "    atrisk_XB = torch.exp(torch.einsum('ijk,k->ij', tensor, beta)).clone()\n",
    "    #atrisk_XB = torch.exp(torch.nonzero(torch.einsum('ijk,k->ij', tensor, beta)).float())\n",
    "    #print(atrisk_XB.shape)\n",
    "    \n",
    "    #Padding zeroes in tensor will turn into 1s with the exp and then skew the sum if not turned into zeroes\n",
    "    atrisk_XB[atrisk_XB == 1] = 0\n",
    "    \n",
    "    #print(atrisk_XB)\n",
    "    #print(torch.where(atrisk_XB==1., 0, atrisk_XB))\n",
    "    #atrisk_XB = torch.where(atrisk_XB.type(torch.LongTensor) == 1, 0, atrisk_XB.type(torch.LongTensor)).float()\n",
    "    loss_atrisk = -torch.log(torch.sum(atrisk_XB, dim=1))\n",
    "    #print(loss_atrisk)\n",
    "    #loss_atrisk = -torch.log((torch.where(atrisk_XB.type(torch.LongTensor)==1, 0, atrisk_XB.type(torch.LongTensor)).sum(dim=1).float()))\n",
    "    #signtens = torch.stack([(tensor[slice, :, 0]==tensor[slice,0,0]) & (tensor[slice, :, 1]==1) for slice in range(tensor.shape[0])])\n",
    "    #signtens = torch.where(signtens == True, -1, 1)\n",
    "    #tensor2 = torch.einsum('ij,ijk->ijk', signtens, tensor)\n",
    "    #tensor2[:, :, :2] = torch.abs(tensor2[:, :, :2]) \n",
    "    #print(loss_atrisk.shape)\n",
    "\n",
    "    loss = torch.sum(loss_event + loss_atrisk)\n",
    "    #loss = torch.sum(loss_atrisk) + torch.sum(loss_event)\n",
    "    \n",
    "    #loss = torch.sum(loss_event.add(loss_atrisk))\n",
    "    return -loss\n",
    "\n",
    "\n",
    "def closure():\n",
    "    optimizer_val2.zero_grad()\n",
    "    loss_val2 = get_loss2(tensor, event_tens, beta_val2)\n",
    "    loss_val2.backward()\n",
    "    return loss_val2\n",
    "\n",
    "optimizer_val2.step(closure)\n",
    "\n",
    "betas_val2 = beta_val2.detach().numpy()\n",
    "betas_val2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
